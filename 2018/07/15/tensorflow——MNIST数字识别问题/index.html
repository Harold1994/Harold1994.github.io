<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>tensorflow——MNIST数字识别问题 | lyyourc</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <link rel="shortcut icon" href="/favicon.ico">
  <link rel="stylesheet" href="/css/app.css">
  <!-- <link rel='stylesheet' href='http://fonts.useso.com/css?family=Source+Code+Pro'> -->
  
</head>

<body>
  <nav class="app-nav">
  
    
      <a href="/.">home</a>
    
  
    
      <a href="/archives">archive</a>
    
  
    
      <a href="/atom.xml">rss</a>
    
  
</nav>

  <main class="post">
  <article>
  <h1 class="article-title">
    <a href="/2018/07/15/tensorflow——MNIST数字识别问题/">tensorflow——MNIST数字识别问题</a>
  </h1>

  <section class="article-meta">
    <p class="article-date">July 15 2018</p>
  </section>

  <section class="article-entry">
    <h4 id="一、-MNIST数据处理"><a href="#一、-MNIST数据处理" class="headerlink" title="一、 MNIST数据处理"></a>一、 MNIST数据处理</h4><p><strong>1. input_data.read_data_seTensorflow函数</strong></p>
<p><strong>功能</strong>: 该函数生成的类能将MNIST数据集划分为train, validation和test三个数据集:</p>
<ul>
<li>train: 55000张图片</li>
<li>validation: 5000张</li>
<li>test: 10000张</li>
</ul>
<p>trian和validation组成了训练数据集. 每一张图片是一个长度为784(=28*28像素)的一维数组.</p>
<a id="more"></a> 
<p><strong>2. mnist.train.next_batch函数</strong></p>
<p>从所有的训练数据集中读取一小部分(batch)用来训练.</p>
<h4 id="二、神经网络训练及不同模型结果对比"><a href="#二、神经网络训练及不同模型结果对比" class="headerlink" title="二、神经网络训练及不同模型结果对比"></a>二、神经网络训练及不同模型结果对比</h4><p><strong>1.Tensorflow训练神经网络</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"><span class="comment">#MNIST数据集相关常数</span></span><br><span class="line">INPUT_NODE = <span class="number">784</span></span><br><span class="line">OUTPUT_NODE = <span class="number">10</span></span><br><span class="line"><span class="comment">#配置神经网络的参数</span></span><br><span class="line">LAYER1_NODE = <span class="number">500</span><span class="comment">#隐藏层节点个数</span></span><br><span class="line">BATCH_SIZE = <span class="number">100</span></span><br><span class="line"></span><br><span class="line">LEARNING_RATE_BASE = <span class="number">0.8</span></span><br><span class="line">LEARNING_RATE_DECAY = <span class="number">0.99</span></span><br><span class="line"></span><br><span class="line">REGULARAZTION_RATE = <span class="number">0.0001</span><span class="comment">#正则化项系数</span></span><br><span class="line">TRAINING_STEPS = <span class="number">5000</span></span><br><span class="line">MOVING_AVERAGE_DECAY = <span class="number">0.99</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#计算神经网络前向传播结果，定义了一个用ReLU激活函数的全连接神经网络</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inference</span><span class="params">(input_tensor, avg_class, weighTensorflow1, biases1, weighTensorflow2, biases2)</span>:</span></span><br><span class="line">    <span class="comment">#如果没有提供滑动平均类，直接用参数当前取值</span></span><br><span class="line">    <span class="keyword">if</span> avg_class == <span class="keyword">None</span>:</span><br><span class="line">        layer1 = tf.nn.relu(</span><br><span class="line">            tf.matmul(input_tensor, weighTensorflow1) + biases1)</span><br><span class="line">        <span class="keyword">return</span> tf.matmul(layer1,weighTensorflow2) + biases2</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        layer1 = tf.nn.relu(</span><br><span class="line">            tf.matmul(input_tensor, avg_class.average(weighTensorflow1)) + avg_class.average(biases1))</span><br><span class="line">        <span class="keyword">return</span> tf.matmul(layer1, avg_class.average(weighTensorflow2)) + avg_class.average(biases2)</span><br><span class="line"></span><br><span class="line"><span class="comment">#训练模型的过程</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(mnist)</span>:</span></span><br><span class="line">    x = tf.placeholder(tf.float32,[<span class="keyword">None</span>, INPUT_NODE], name=<span class="string">'x-input'</span>)</span><br><span class="line">    y_ = tf.placeholder(tf.float32,[<span class="keyword">None</span>, OUTPUT_NODE],name=<span class="string">'y-inpit'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 隐藏层参数</span></span><br><span class="line">    weighTensorflow1 = tf.Variable(tf.truncated_normal([INPUT_NODE,LAYER1_NODE], stddev=<span class="number">0.1</span>))</span><br><span class="line">    biases1 = tf.Variable(tf.constant(<span class="number">0.1</span>, shape=[LAYER1_NODE]))</span><br><span class="line">    <span class="comment">#输出层参数</span></span><br><span class="line">    weighTensorflow2 = tf.Variable(tf.truncated_normal([LAYER1_NODE,OUTPUT_NODE],stddev=<span class="number">0.1</span>))</span><br><span class="line">    biases2 = tf.Variable(tf.constant(<span class="number">0.1</span>, shape=[OUTPUT_NODE]))</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#计算在当前参数下前向传播的结果，不使用滑动平均</span></span><br><span class="line">    y = inference(x, <span class="keyword">None</span>, weighTensorflow1, biases1, weighTensorflow2, biases2)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#定义存储训练轮数的变量，不需要计算滑动平均，因此指定为不可训练变量</span></span><br><span class="line">    global_step = tf.Variable(<span class="number">0</span>, trainable=<span class="keyword">False</span>)</span><br><span class="line">    <span class="comment"># 初始化滑动平均类</span></span><br><span class="line">    variable_average = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)</span><br><span class="line">    <span class="comment">#在所有代表神经网络参数的变量上使用滑动平均</span></span><br><span class="line">    variable_average_op = variable_average.apply(tf.trainable_variables())</span><br><span class="line">    <span class="comment">#计算使用了滑动平均之后的前向传播结果</span></span><br><span class="line">    average_y = inference(x, variable_average, weighTensorflow1, biases1, weighTensorflow2, biases2)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#交叉熵，当分类问题中只有一个正确答案时，sparse_softmax_cross_entropy_with_logiTensorflow函数</span></span><br><span class="line">    <span class="comment">#可以加速交叉熵的计算。标准答案是长度为10的一维数组，而该函数需要提供的是一个正确答案的数字，因此用tf.argmax函数</span></span><br><span class="line">    <span class="comment">#来得到正确答案对应类别编号</span></span><br><span class="line">    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logiTensorflow(logiTensorflow=y, labels=tf.argmax(y_,<span class="number">1</span>))</span><br><span class="line">    <span class="comment">#计算当前batch中所有样例的平均交叉熵</span></span><br><span class="line">    cross_entropy_mean = tf.reduce_mean(cross_entropy)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#L2损失函数</span></span><br><span class="line">    regularizer = tf.contrib.layers.l2_regularizer(REGULARAZTION_RATE)</span><br><span class="line">    regularization = regularizer(weighTensorflow1) + regularizer(weighTensorflow2)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#总损失等于交叉熵损失加正则化损失</span></span><br><span class="line">    loss = cross_entropy_mean + regularization</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#指数衰减的学习率</span></span><br><span class="line">    learning_rate = tf.train.exponential_decay(</span><br><span class="line">        LEARNING_RATE_BASE,<span class="comment">#基础学习率</span></span><br><span class="line">        global_step, <span class="comment">#当前迭代轮数</span></span><br><span class="line">        mnist.train.num_examples/BATCH_SIZE,<span class="comment">#过完所有数据需要的迭代次数</span></span><br><span class="line">        LEARNING_RATE_DECAY)<span class="comment">#衰减速度</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 优化损失函数</span></span><br><span class="line">    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(</span><br><span class="line">        loss, global_step=global_step)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#在训练神经网络模型时，每过一遍数据既需要通过反向传播来更新神经网络中的参数，</span></span><br><span class="line">    <span class="comment">#又要更新每一个参数的滑动平均值。为了一次完成多个操作，Tensorflow提供了tf.control_dependencies</span></span><br><span class="line">    <span class="comment">#和tf.group两种机制，下面两行等价于：train_op = tf.group(train_step, variable_average_op)</span></span><br><span class="line">    <span class="keyword">with</span> tf.control_dependencies([train_step, variable_average_op]):</span><br><span class="line">        train_op = tf.no_op(name = <span class="string">'train'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">#检验使用了滑动平均模型的神经网络前向传播结果是否正确，tf.argmax(average_y,1)</span></span><br><span class="line">    <span class="comment">#计算每个样例的预测答案，第二个参数1表示选取最大值的操作仅在第一个维度进行。于是得到结果是一个长度为batch</span></span><br><span class="line">    <span class="comment">#的一维bool数组</span></span><br><span class="line">    correct_prediction = tf.equal(tf.argmax(average_y,<span class="number">1</span>), tf.argmax(y_,<span class="number">1</span>))</span><br><span class="line">    <span class="comment">#将bool数组转换为实数型，然后计算均值，即正确率</span></span><br><span class="line">    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        tf.initialize_all_variables().run()</span><br><span class="line">        <span class="comment">#验证数据</span></span><br><span class="line">        validate_feed = &#123;x: mnist.validation.images,</span><br><span class="line">                         y_:mnist.validation.labels&#125;</span><br><span class="line">        <span class="comment">#测试数据</span></span><br><span class="line">        test_feed = &#123;x: mnist.test.images, </span><br><span class="line">                     y_:mnist.test.labels&#125;</span><br><span class="line">        <span class="comment">#迭代训练</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(TRAINING_STEPS):</span><br><span class="line">            <span class="comment">#每1000轮输出一次验证集上的测试结果</span></span><br><span class="line">            <span class="keyword">if</span> i%<span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">                validate_acc = sess.run(accuracy, feed_dict = validate_feed)</span><br><span class="line">                print(<span class="string">"After %d training steps(s), validation accuracy using average model is  %g"</span>%(i,validate_acc))</span><br><span class="line">            xs,ys = mnist.train.next_batch(BATCH_SIZE)</span><br><span class="line">            sess.run(train_op, feed_dict = &#123;x:xs, y_:ys&#125;)</span><br><span class="line">        test_acc = sess.run(accuracy, feed_dict=test_feed)</span><br><span class="line">        print(<span class="string">"After %d training steps(s), test accuracy using average model is  %g"</span>%(TRAINING_STEPS,test_acc))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(argv=None)</span>:</span></span><br><span class="line">    mnist = input_data.read_data_seTensorflow(<span class="string">'/data/'</span>, one_hot=<span class="keyword">True</span>)</span><br><span class="line">    train(mnist)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    tf.app.run()</span><br></pre></td></tr></table></figure>
<p><strong>2. 使用验证集判断模型效果</strong></p>
<p>​    在上一节程序的开始设置了初始学习率，隐藏层节点数量，迭代轮数等参数，这些参数的取值是需要通过实验调整的，不能使用模型在测试数据上的效果选择参数，使用测试集可能导致模型过度拟合，从而失去对未知数据的预测能力，因此一般从训练集中选一部分做验证集，验证数据就可以评判不同参数取值下模型的表现。除了使用验证集，还可以使用交叉验证来验证模型效果，但是神经网络训练时间本身较长，用cv会花费大量时间，所以在海量数据情况下，一般更多使用验证集来评估模型效果。</p>
<h4 id="三、变量管理"><a href="#三、变量管理" class="headerlink" title="三、变量管理"></a>三、变量管理</h4><p>​    在inference函数中传入了计算前向传播结果所需要的参数，当神经网络更复杂，参数更多的时候，就需要更好的方式来传递和管理神经网络中的参数。Tensorflow提供了通过<strong>变量名</strong>来创建和获取一个变量的机制，这样在不同的函数中可以直接通过变量的名字来使用变量。主要通过tf.get_variable和tf.variable_scope函数来实现。</p>
<p>除了tf.Variable，Tensorflow还可以用tf.get_variable函数来创建或获取变量，它和tf.Variable基本等价。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">v = tf.get_variable(<span class="string">"v"</span>, shape=[<span class="number">1</span>], </span><br><span class="line">                    initializer=tf.constant_initializer(<span class="number">1.0</span>))</span><br><span class="line">v = tf.Variable(tf.constant(<span class="number">1.0</span>,shape=[<span class="number">1</span>]), name=<span class="string">"v"</span>)</span><br></pre></td></tr></table></figure>
<p>Tensorflow提供了七种不同的初始化函数：</p>
<p><img src="http://p5s7d12ls.bkt.clouddn.com/18-7-15/22335461.jpg" alt=""></p>
<p>​    tf.get_variable和tf.Variable函数最大的区别是制定变量名称的参数，后者的name=””参数可选,而对于tf.get_variable，变量名是一个必填的参数。tf.get_variable会根据这个名字去创建或获取变量，根据tf.get_variable创建的变量的变量名都是不可重复的。</p>
<p>​    如果需要通过tf.get_variable获取一个已经创建的变量，需要通过tf.variable_scope函数来生成一个上下文管理器，并明确指定在这个上下文管理器中，tf.get_variable将直接获取已经生成的变量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#在名字为foo的命名空间中创建名字为v的变量</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"foo"</span>):</span><br><span class="line">    v = tf.get_variable(</span><br><span class="line">        <span class="string">"v"</span>, shape=[<span class="number">1</span>], initializer=tf.constant_initializer(<span class="number">1.0</span>))</span><br><span class="line">    </span><br><span class="line"><span class="comment">#因为在foo中已经存在名字为v的变量，所以下面会报错</span></span><br><span class="line"><span class="comment">#riable foo/v already exisTensorflow, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># with tf.variable_scope("foo"):</span></span><br><span class="line"><span class="comment">#     tf.get_variable("v",[1])</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#在声明上下文管理器时，将参数reuse设为true，这样tf.get_variable函数将直接获取以声明的变量</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"foo"</span>,reuse=<span class="keyword">True</span>):</span><br><span class="line">    v1 = tf.get_variable(<span class="string">"v"</span>,[<span class="number">1</span>])</span><br><span class="line">    print(v==v1) <span class="comment">#True</span></span><br></pre></td></tr></table></figure>
<p>Tensorflow中，tf.variable_scope可以嵌套：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"root"</span>):</span><br><span class="line">    <span class="comment">#可以tf.get_variable_scope().reuse获取上下文中reuse参数的取值</span></span><br><span class="line">    print(tf.get_variable_scope().reuse)<span class="comment">#False,最外层reuse是False</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"foo"</span>, reuse=<span class="keyword">True</span>):</span><br><span class="line">        print(tf.get_variable_scope().reuse)<span class="comment">#True</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">"bar"</span>):</span><br><span class="line">            print(tf.get_variable_scope().reuse)<span class="comment">#True,虽然bar没有制定</span></span><br><span class="line">                                                <span class="comment">#reuse,这时会和外层foo保持一致</span></span><br><span class="line">    print(tf.get_variable_scope().reuse)<span class="comment">#False，退出reuse为True的上下文后，又回到False</span></span><br></pre></td></tr></table></figure>
<p>tf.variable_scope函数生成的上下文也会创建一个Tensorflow中的命名空间，在命名空间内创建的变量名称都会带上命名空间作为前缀，tf.variable_scope提供了一个管理变量命名空间的方式。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">v1 = tf.get_variable(<span class="string">"v"</span>,[<span class="number">1</span>])</span><br><span class="line">print(v1.name) <span class="comment">#v:0,v是变量名，0表示这个变量是生成变量这个运算的第一个结果</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"foo"</span>, reuse=<span class="keyword">True</span>):</span><br><span class="line">    v2 = tf.get_variable(<span class="string">"v"</span>,[<span class="number">1</span>])</span><br><span class="line">    print(v2.name) <span class="comment">#foo/v:0 在/前加了命名空间名称</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"foo"</span>,reuse=<span class="keyword">True</span>):</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"bar"</span>):</span><br><span class="line">        v3 = tf.get_variable(<span class="string">"v"</span>,[<span class="number">1</span>])</span><br><span class="line">        print(v3.name)<span class="comment">#foo/bar/v:0</span></span><br><span class="line">    </span><br><span class="line">    v4 = tf.get_variable(<span class="string">"v"</span>,[<span class="number">1</span>])</span><br><span class="line">    print(v4.name)<span class="comment">#foo/v:0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">""</span>, reuse=<span class="keyword">True</span>):</span><br><span class="line">    <span class="comment">#可以通过带命名空间名称的变量名来获取其他命名空间下的变量</span></span><br><span class="line">    v5 = tf.get_variable(<span class="string">"foo/bar/v"</span>,[<span class="number">1</span>])</span><br><span class="line">    print(v5 == v3)<span class="comment">#True</span></span><br><span class="line">    v6 = tf.get_variable(<span class="string">"foo/v"</span>,[<span class="number">1</span>])</span><br><span class="line">    print(v6 == v4)</span><br></pre></td></tr></table></figure>
  </section>
</article>

  <div class="sharing grid">
  <section class="profile grid-item grid">
    <img class="avatar" src="http://7xrcp8.com1.z0.glb.clouddn.com/avatar.png" alt="avatar" />
    <div class="grid-item">
      <p class="title"> lyyourc </p>
      <p class="subtitle"> You Are The JavaScript In My HTML </p>
    <div>
  </section>

  <section class="share-btns">
    <!-- <p> share it if you like it~ </p> -->
    <a
  class="twitter-share-button"
  data-size="large"
  data-via="DrakeLeung"
  href="https://twitter.com/intent/tweet?text= id="一、-MNIST数据处理"><"
>
  Tweet
</a>

<script>
  window.twttr = (function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0],
    t = window.twttr || {};
  if (d.getElementById(id)) return t;
  js = d.createElement(s);
  js.id = id;
  js.src = "https://platform.twitter.com/widgets.js";
  js.async = true;
  fjs.parentNode.insertBefore(js, fjs);

  t._e = [];
  t.ready = function(f) {
    t._e.push(f);
  };

  return t;
}(document, "script", "twitter-wjs"));
</script>

  </section>
</div>


  
    
<section class="article-comment">
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
</section>

<script>
  var disqus_shortname = 'drakeleung';
  
  var disqus_url = '//harold.me/2018/07/15/tensorflow——MNIST数字识别问题/';
  
  (function(){
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>


  
</main>

</body>
</html>
