<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>MLlib-特征的提取、转换与选择 | lyyourc</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <link rel="shortcut icon" href="/favicon.ico">
  <link rel="stylesheet" href="/css/app.css">
  <!-- <link rel='stylesheet' href='http://fonts.useso.com/css?family=Source+Code+Pro'> -->
  
</head>

<body>
  <nav class="app-nav">
  
    
      <a href="/.">home</a>
    
  
    
      <a href="/archives">archive</a>
    
  
    
      <a href="/atom.xml">rss</a>
    
  
</nav>

  <main class="post">
  <article>
  <h1 class="article-title">
    <a href="/2018/06/26/MLlib-特征的提取、转换与选择/">MLlib-特征的提取、转换与选择</a>
  </h1>

  <section class="article-meta">
    <p class="article-date">June 26 2018</p>
  </section>

  <section class="article-entry">
    <p>本博客介绍使用功能的算法，大致分为以下几类：</p>
<ul>
<li>提取：从“原始”数据中提取特征</li>
<li>转换：缩放，转换或修改特征</li>
<li>选择：从较大的一组特征中选择一个子集</li>
<li>局部敏感哈希（LSH）：这类算法将特征变换的方面与其他算法相结合。</li>
</ul>
<a id="more"></a> 
<h4 id="一、特征提取"><a href="#一、特征提取" class="headerlink" title="一、特征提取"></a>一、特征提取</h4><p><strong>1.TF-IDF</strong></p>
<p><a href="http://en.wikipedia.org/wiki/Tf%E2%80%93idf" target="_blank" rel="noopener">词频（Term Frequency）- 逆向文档频率（Inverse Document Frequency）</a> 是一种在文本挖掘中广泛使用的特征向量化方法，以反映一个单词在语料库中的重要性。定义：<em>t</em> 表示由一个单词，<em>d</em> 表示一个文档，<em>D</em> 表示语料库（corpus），词频 <em>TF(t,d)</em> 表示某一个给定的单词 <em>t</em> 出现在文档 <em>d</em> 中的次数， 而文档频率 <em>DF(t,D)</em> 表示包含单词 <em>t</em>  的文档次数。如果我们只使用词频 <em>TF</em> 来衡量重要性，则很容易过分强调出现频率过高并且文档包含少许信息的单词，例如，’a’，’the’，和 ‘of’。如果一个单词在整个语料库中出现的非常频繁，这意味着它并没有携带特定文档的某些特殊信息（换句话说，该单词对整个文档的重要程度低）。逆向文档频率是一个数字量度，表示一个单词提供了多少信息：</p>
<p>​                                                                  $IDF(t,D) = \log \frac {|D| + 1}{DF(t,D) + 1}$</p>
<p> 其中，|<em>D</em>| 是在语料库中文档总数。由于使用对数，所以如果一个单词出现在所有的文件，其IDF值变为0。注意，应用平滑项以避免在语料库之外的项除以零（为了防止分母为0，分母需要加1）。因此，TF-IDF测量只是TF和IDF的产物：（对TF-IDF定义为TF和IDF的乘积）</p>
<p>​                                                      $TD-IDF(t,d,D) = TF(t,d) \cdot IDF(t,D)$</p>
<p> 关于词频TF和文档频率DF的定义有多种形式。在MLlib，我们分离TF和IDF，使其灵活。</p>
<p><strong>TF（词频Term Frequency）</strong>：<code>HashingTF</code>与<code>CountVectorizer都可以</code>用于生成词频TF向量。</p>
<p>HashingTF是一个需要特征词集的转换器（Transformer），它可以将这些集合转换成固定长度的特征向量。在文本处理中，“特征词集”有一系列的特征词构成。<code>HashingTF</code>利用<a href="http://en.wikipedia.org/wiki/Feature_hashing" target="_blank" rel="noopener">hashing trick</a>，原始特征（raw feature）通过应用哈希函数映射到索引中。这里使用的哈希函数是<a href="https://en.wikipedia.org/wiki/MurmurHash" target="_blank" rel="noopener">murmurHash 3</a>。然后根据映射的索引计算词频。这种方法避免了计算全局特征词对索引映射的需要，这对于大型语料库来说可能是昂贵的，但是它具有潜在的哈希冲突，其中不同的原始特征可以在散列之后变成相同的特征词。为了减少碰撞的机会，我们可以增加目标特征维度，即哈希表的桶数。由于使用简单的模数将散列函数转换为列索引，建议使用两个幂作为特征维，否则不会将特征均匀地映射到列。默认功能维度为$2^{18} = 262144$。可选的二进制切换参数控制词频计数,当设置为true时，所有非零频率计数设置为1。这对于模拟二进制而不是整数的离散概率模型尤其有用。</p>
<p><strong>IDF（逆向文档频率）</strong>：IDF是一个适合数据集并生成IDFModel的评估器（<code>Estimator），</code>IDFModel获取特征向量（通常由HashingTF或CountVectorizer创建）并缩放每列。直观地说，它下调了在语料库中频繁出现的列。</p>
<p>CountVectorizer将文本文档转换为关键词计数的向量。</p>
<p>在下面的代码段中，我们从一组句子开始。我们使用Tokenizer将每个句子分成单词。对于每个句子（词袋，词集：bag of words），我们使用HashingTF将该句子哈希成特征向量。我们使用IDF来重新缩放特征向量；这通常会在使用文本作为功能时提高性能。然后，我们的特征向量可以被传递给学习算法。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TfIdfExample</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> spark= <span class="type">SparkSession</span>.builder()</span><br><span class="line">      .master(<span class="string">"local"</span>)</span><br><span class="line">      .appName(<span class="string">"TfIdfExample"</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line">    <span class="keyword">val</span> sentenceData = spark.createDataFrame(<span class="type">Seq</span>(</span><br><span class="line">      (<span class="number">0.0</span>, <span class="string">"Hi I heard about Spark"</span>),</span><br><span class="line">      (<span class="number">0.0</span>, <span class="string">"I wish Java could use case classes"</span>),</span><br><span class="line">      (<span class="number">1.0</span>, <span class="string">"Logistic regression models are neat"</span>)</span><br><span class="line">    )).toDF(<span class="string">"label"</span>,<span class="string">"sentence"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> tokenizer = <span class="keyword">new</span> <span class="type">Tokenizer</span>().setInputCol(<span class="string">"sentence"</span>).setOutputCol(<span class="string">"words"</span>)</span><br><span class="line">    <span class="keyword">val</span> wordsData = tokenizer.transform(sentenceData)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> hashingTF =<span class="keyword">new</span> <span class="type">HashingTF</span>()</span><br><span class="line">      .setInputCol(<span class="string">"words"</span>).setOutputCol(<span class="string">"rawFeatures"</span>).setNumFeatures(<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> featurizedData = hashingTF.transform(wordsData)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> idf = <span class="keyword">new</span> <span class="type">IDF</span>().setInputCol(<span class="string">"rawFeatures"</span>).setOutputCol(<span class="string">"features"</span>)</span><br><span class="line">    <span class="keyword">val</span> idfModel = idf.fit(featurizedData)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rescaledData = idfModel.transform(featurizedData)</span><br><span class="line">    rescaledData.select(<span class="string">"label"</span>,<span class="string">"features"</span>).foreach(println(_))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>2.Word2Vec</strong></p>
<p>Word2Vec是一个Estimator(评估器)，它采用表示文档的单词序列，并训练一个Word2VecModel。 该模型将每个单词映射到一个唯一的固定大小向量。 Word2VecModel使用文档中所有单词的平均值将每个文档转换为向量; 该向量然后可用作预测，文档相似性计算等功能。</p>
<p>在下面的代码段中，我们从一组文档开始，每一个文档都用一个单词序列表示。 对于每个文档，我们将其转换为特征向量。 然后可以将该特征向量传递给学习算法。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Word2VecExample</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> spark= <span class="type">SparkSession</span>.builder()</span><br><span class="line">      .master(<span class="string">"local"</span>)</span><br><span class="line">      .appName(<span class="string">"Word2VecExample"</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> documentDF = spark.createDataFrame(<span class="type">Seq</span>(</span><br><span class="line">      <span class="string">"Hi I heard about Spark"</span>.split(<span class="string">" "</span>),</span><br><span class="line">      <span class="string">"I wish Java could use case classes"</span>.split(<span class="string">" "</span>),</span><br><span class="line">      <span class="string">"Logistic regression models are neat"</span>.split(<span class="string">" "</span>)</span><br><span class="line">    ).map(<span class="type">Tuple1</span>.apply)).toDF(<span class="string">"text"</span>)</span><br><span class="line"></span><br><span class="line">     <span class="keyword">val</span> word2Vec = <span class="keyword">new</span> <span class="type">Word2Vec</span>()</span><br><span class="line">      .setInputCol(<span class="string">"text"</span>)</span><br><span class="line">      .setOutputCol(<span class="string">"result"</span>)</span><br><span class="line">      .setVectorSize(<span class="number">3</span>)</span><br><span class="line">      .setMinCount(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> model = word2Vec.fit(documentDF)</span><br><span class="line">    <span class="keyword">val</span> result = model.transform(documentDF)</span><br><span class="line">    result.collect().foreach&#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">Row</span>(text:<span class="type">Seq</span>[_],feature:<span class="type">Vector</span>) =&gt;</span><br><span class="line">        println(<span class="string">s"Text : [<span class="subst">$&#123;text.mkString(", ")&#125;</span>] =&gt; \nVector: <span class="subst">$feature</span>"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>3.CountVectorizer</strong></p>
<p>CountVectorizer和CountVectorizerModel旨在帮助将文本文档集合转换为标记数的向量。 当先验词典不可用时，CountVectorizer可以用作估计器来提取词汇表，并生成CountVectorizerModel。 该模型通过词汇生成文档的稀疏表示，然后可以将其传递给其他算法，如LDA。</p>
<p>在拟合过程中，CountVectorizer将选择通过语料库按术语频率排序的top前几vocabSize词。 可选参数minDF还通过指定术语必须出现以包含在词汇表中的文档的最小数量（或小于1.0）来影响拟合过程。 另一个可选的二进制切换参数控制输出向量。 如果设置为true，则所有非零计数都设置为1.对于模拟二进制而不是整数的离散概率模型，这是非常有用的。假设我们有如下的DataFrame包含id和texts两列：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"> id | texts</span><br><span class="line">----|----------</span><br><span class="line"> 0  | Array(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;)</span><br><span class="line"> 1  | Array(&quot;a&quot;, &quot;b&quot;, &quot;b&quot;, &quot;c&quot;, &quot;a&quot;)</span><br></pre></td></tr></table></figure>
<p>文本中的每一行都是Array[String]类型的文档。调用CountVectorizer的拟合产生一个具有词汇表（a, b, c）的CountVectorizerModel。然后转换后的输出列 包含“向量”这一列：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"> id | texts                           | vector</span><br><span class="line">----|---------------------------------|---------------</span><br><span class="line"> 0  | Array(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;)            | (3,[0,1,2],[1.0,1.0,1.0])</span><br><span class="line"> 1  | Array(&quot;a&quot;, &quot;b&quot;, &quot;b&quot;, &quot;c&quot;, &quot;a&quot;)  | (3,[0,1,2],[2.0,2.0,1.0])</span><br></pre></td></tr></table></figure>
<p>每个向量表示文档在词汇表上的标记数。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">CountVectorizerExample</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> spark= <span class="type">SparkSession</span>.builder()</span><br><span class="line">      .master(<span class="string">"local"</span>)</span><br><span class="line">      .appName(<span class="string">"Word2VecExample"</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> df = spark.createDataFrame(<span class="type">Seq</span>(</span><br><span class="line">      (<span class="number">0</span>, <span class="type">Array</span>(<span class="string">"a"</span>, <span class="string">"b"</span>, <span class="string">"c"</span>,<span class="string">"d"</span>)),</span><br><span class="line">      (<span class="number">1</span>, <span class="type">Array</span>(<span class="string">"a"</span>, <span class="string">"b"</span>, <span class="string">"b"</span>, <span class="string">"c"</span>, <span class="string">"a"</span>,<span class="string">"d"</span>,<span class="string">"d"</span>))</span><br><span class="line">    )).toDF(<span class="string">"id"</span>, <span class="string">"words"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> cvModel : <span class="type">CountVectorizerModel</span> = <span class="keyword">new</span> <span class="type">CountVectorizer</span>()</span><br><span class="line">      .setInputCol(<span class="string">"words"</span>)</span><br><span class="line">      .setOutputCol(<span class="string">"features"</span>)</span><br><span class="line">      .setVocabSize(<span class="number">3</span>)</span><br><span class="line">      .setMinDF(<span class="number">2</span>)</span><br><span class="line">      .fit(df)</span><br><span class="line">   <span class="comment">//也可以使用先验词汇</span></span><br><span class="line">    <span class="keyword">val</span> cvm = <span class="keyword">new</span> <span class="type">CountVectorizerModel</span>(<span class="type">Array</span>(<span class="string">"a"</span>,<span class="string">"b"</span>,<span class="string">"c"</span>))</span><br><span class="line">      .setInputCol(<span class="string">"words"</span>)</span><br><span class="line">      .setOutputCol(<span class="string">"result"</span>)</span><br><span class="line"></span><br><span class="line">    cvModel.transform(df).show(<span class="literal">false</span>)</span><br><span class="line"><span class="comment">//    cvm.transform(df).show(false)</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="二、特征变换"><a href="#二、特征变换" class="headerlink" title="二、特征变换"></a>二、特征变换</h4><p><strong>1.Tokenizer分词器</strong></p>
<p><a href="http://en.wikipedia.org/wiki/Lexical_analysis#Tokenization" target="_blank" rel="noopener">Tokenization</a>（文本符号化）是将文本 （如一个句子）拆分成单词的过程。（在Spark ML中）<a href="http://spark.apache.org/docs/2.0.2/api/scala/index.html#org.apache.spark.ml.feature.Tokenizer" target="_blank" rel="noopener">Tokenizer</a>（分词器）提供此功能。下面的示例演示如何将句子拆分为词的序列。</p>
<p><a href="http://spark.apache.org/docs/2.0.2/api/scala/index.html#org.apache.spark.ml.feature.RegexTokenizer" target="_blank" rel="noopener">RegexTokenizer</a> 提供了（更高级的）基于正则表达式 (regex) 匹配的（对句子或文本的）单词拆分。默认情况下，参数”pattern”(默认的正则表达式: <code>&quot;\\s+&quot;</code>) 作为分隔符用于拆分输入的文本。或者，用户可以将参数“gaps”设置为 false ，指定正则表达式”pattern”表示”tokens”，而不是分隔符，这样作为划分结果找到的所有匹配项。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TokenizerExample</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder()</span><br><span class="line">      .master(<span class="string">"local"</span>)</span><br><span class="line">      .appName(<span class="string">"TokenizerExample"</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sentenceDataFrame = spark.createDataFrame(<span class="type">Seq</span>(</span><br><span class="line">      (<span class="number">0</span>, <span class="string">"Hi I heard about Spark"</span>),</span><br><span class="line">      (<span class="number">1</span>, <span class="string">"I wish Java could use case classes"</span>),</span><br><span class="line">      (<span class="number">2</span>, <span class="string">"Logistic,regression,models,are,neat"</span>)</span><br><span class="line">    )).toDF(<span class="string">"id"</span>, <span class="string">"sentence"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> tokenizer = <span class="keyword">new</span> <span class="type">Tokenizer</span>().setInputCol(<span class="string">"sentence"</span>).setOutputCol(<span class="string">"words"</span>)</span><br><span class="line">    <span class="keyword">val</span> regexTokenizer = <span class="keyword">new</span> <span class="type">RegexTokenizer</span>()</span><br><span class="line">      .setInputCol(<span class="string">"sentence"</span>)</span><br><span class="line">      .setOutputCol(<span class="string">"words"</span>)</span><br><span class="line">      .setPattern(<span class="string">"\\w"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> countTokens = udf &#123; (words: <span class="type">Seq</span>[<span class="type">String</span>]) =&gt; words.length &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> tokenized = tokenizer.transform(sentenceDataFrame)</span><br><span class="line">    tokenized.select(<span class="string">"sentence"</span>, <span class="string">"words"</span>)</span><br><span class="line">      .withColumn(<span class="string">"tokens"</span>, countTokens(col(<span class="string">"words"</span>))).show(<span class="literal">false</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> regexTokenized = regexTokenizer.transform(sentenceDataFrame)</span><br><span class="line">    regexTokenized.select(<span class="string">"sentence"</span>, <span class="string">"words"</span>)</span><br><span class="line">      .withColumn(<span class="string">"tokens"</span>, countTokens(col(<span class="string">"words"</span>))).show(<span class="literal">false</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>2.StopWordsRemover去停用词</strong></p>
<p><a href="https://en.wikipedia.org/wiki/Stop_words" target="_blank" rel="noopener">Stop words （停用字）</a>是（在文档中）频繁出现，但未携带太多意义的词语，它们不应该参与算法运算。</p>
<p> <code>StopWordsRemover（的作用是）将</code>输入的字符串 （如分词器 <a href="http://spark.apache.org/docs/2.0.2/ml-features.html#tokenizer" target="_blank" rel="noopener">Tokenizer</a> 的输出）中的停用字删除（后输出）。停用字表由 <code>stopWords</code>参数指定。对于某些语言的默认停止词是通过调用 <code>StopWordsRemover.loadDefaultStopWords(language) 设置的</code>，可用的选项为”丹麦”，”荷兰语”、”英语”、”芬兰语”，”法国”，”德国”、”匈牙利”、”意大利”、”挪威”、”葡萄牙”、”俄罗斯”、”西班牙”、”瑞典”和”土耳其”。布尔型参数 <code>caseSensitive</code>指示是否区分大小写 （默认为否）。</p>
<p>假设有如下DataFrame，有id和raw两列：</p>
<table>
<thead>
<tr>
<th style="text-align:center">id</th>
<th>raw</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0</td>
<td>[I, saw, the, red, baloon]</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td>[Mary, had, a, little, lamb]</td>
</tr>
</tbody>
</table>
<p>通过对 raw 列调用 StopWordsRemover，我们可以得到筛选出的结果列如下：</p>
<table>
<thead>
<tr>
<th style="text-align:center">id</th>
<th>raw</th>
<th>filtered</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0</td>
<td>[I, saw, the, red, baloon]</td>
<td>[saw, red, baloon]</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td>[Mary, had, a, little, lamb]</td>
<td>[Mary, little, lamb]</td>
</tr>
</tbody>
</table>
<p>其中，“I”, “the”, “had”以及“a”被移除。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StopWordsRemoverExample</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder()</span><br><span class="line">      .master(<span class="string">"local"</span>)</span><br><span class="line">      .appName(<span class="string">"TokenizerExample"</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> remover = <span class="keyword">new</span> <span class="type">StopWordsRemover</span>()</span><br><span class="line">      .setInputCol(<span class="string">"raw"</span>)</span><br><span class="line">      .setOutputCol(<span class="string">"filtered"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> dataSet = spark.createDataFrame(<span class="type">Seq</span>(</span><br><span class="line">      (<span class="number">0</span>, <span class="type">Seq</span>(<span class="string">"I"</span>, <span class="string">"saw"</span>, <span class="string">"the"</span>, <span class="string">"red"</span>, <span class="string">"baloon"</span>)),</span><br><span class="line">      (<span class="number">1</span>, <span class="type">Seq</span>(<span class="string">"Mary"</span>, <span class="string">"had"</span>, <span class="string">"a"</span>, <span class="string">"little"</span>, <span class="string">"lamb"</span>))</span><br><span class="line">    )).toDF(<span class="string">"id"</span>, <span class="string">"raw"</span>)</span><br><span class="line"></span><br><span class="line">    remover.transform(dataSet).show(<span class="literal">false</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>3.n-gram N元模型</strong></p>
<p>一个 <a href="https://en.wikipedia.org/wiki/N-gram" target="_blank" rel="noopener">n-gram</a>是一个长度为n（整数）的字的序列。NGram可用于将输入特征转换成n-grams。</p>
<p>N-Gram 的输入为一系列的字符串（例如：<a href="http://spark.apache.org/docs/2.0.2/ml-features.html#tokenizer" target="_blank" rel="noopener">Tokenizer</a>分词器的输出）。参数 n 表示每个 n-gram 中单词（terms）的数量。输出将由 n-gram 序列组成，其中每个 n-gram 由空格分隔的 n 个连续词的字符串表示。如果输入的字符串序列少于n个单词，NGram 输出为空。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">NGramExample</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder()</span><br><span class="line">      .master(<span class="string">"local"</span>)</span><br><span class="line">      .appName(<span class="string">"NGramExample"</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> wordDataFrame = spark.createDataFrame(<span class="type">Seq</span>(</span><br><span class="line">      (<span class="number">0</span>, <span class="type">Array</span>(<span class="string">"Hi"</span>, <span class="string">"I"</span>, <span class="string">"heard"</span>, <span class="string">"about"</span>, <span class="string">"Spark"</span>)),</span><br><span class="line">      (<span class="number">1</span>, <span class="type">Array</span>(<span class="string">"I"</span>, <span class="string">"wish"</span>, <span class="string">"Java"</span>, <span class="string">"could"</span>, <span class="string">"use"</span>, <span class="string">"case"</span>, <span class="string">"classes"</span>)),</span><br><span class="line">      (<span class="number">2</span>, <span class="type">Array</span>(<span class="string">"Logistic"</span>, <span class="string">"regression"</span>, <span class="string">"models"</span>, <span class="string">"are"</span>, <span class="string">"neat"</span>))</span><br><span class="line">    )).toDF(<span class="string">"id"</span>, <span class="string">"words"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> ngram = <span class="keyword">new</span> <span class="type">NGram</span>().setN(<span class="number">2</span>).setInputCol(<span class="string">"words"</span>).setOutputCol(<span class="string">"ngrams"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> ngramDataFrame = ngram.transform(wordDataFrame)</span><br><span class="line">    ngramDataFrame.select(<span class="string">"ngrams"</span>).show(<span class="literal">false</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">输出：</span><br><span class="line">+------------------------------------------------------------------+</span><br><span class="line">|ngrams                                                            |</span><br><span class="line">+------------------------------------------------------------------+</span><br><span class="line">|[<span class="type">Hi</span> <span class="type">I</span>, <span class="type">I</span> heard, heard about, about <span class="type">Spark</span>]                         |</span><br><span class="line">|[<span class="type">I</span> wish, wish <span class="type">Java</span>, <span class="type">Java</span> could, could use, use <span class="keyword">case</span>, <span class="keyword">case</span> classes]|</span><br><span class="line">|[<span class="type">Logistic</span> regression, regression models, models are, are neat]    |</span><br><span class="line">+------------------------------------------------------------------+</span><br></pre></td></tr></table></figure>
<p><strong>4.Binary二值化</strong></p>
<p>Binarization （二值化）是将数值特征阈值化为二进制（0/1）特征的过程。</p>
<p>Binarizer（ML提供的二元化方法）二元化涉及的参数有 inputCol（输入）、outputCol（输出）以及threshold（阀值）。（输入的）特征值大于阀值将二值化为1.0，特征值小于等于阀值将二值化为0.0。inputCol 支持向量（Vector）和双精度（Double）类型。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">BinaryExample</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder()</span><br><span class="line">      .appName(<span class="string">"BinaryExample"</span>)</span><br><span class="line">      .master(<span class="string">"local"</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> data = <span class="type">Array</span>((<span class="number">0</span>,<span class="number">0.1</span>),(<span class="number">1</span>,<span class="number">0.8</span>),(<span class="number">2</span>,<span class="number">0.2</span>))</span><br><span class="line">    <span class="keyword">val</span> dataFrame = spark.createDataFrame(data).toDF(<span class="string">"id"</span>, <span class="string">"feature"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> binarizer : <span class="type">Binarizer</span> = <span class="keyword">new</span> <span class="type">Binarizer</span>()</span><br><span class="line">      .setInputCol(<span class="string">"feature"</span>)</span><br><span class="line">      .setOutputCol(<span class="string">"binarized_feature"</span>)</span><br><span class="line">      .setThreshold(<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> binarizedDataFrame = binarizer.transform(dataFrame)</span><br><span class="line">    println(<span class="string">s"Binarizer output with Threshold = <span class="subst">$&#123;binarizer.getThreshold&#125;</span>"</span>)</span><br><span class="line">    binarizedDataFrame.show()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>5.主成分分析</strong></p>
<p>PCA 是使用正交变换将可能相关变量的一组观察值转换为称为主成分的线性不相关变量的值的一组统计过程。 PCA 类训练使用 PCA 将向量投影到低维空间的模型。下面的例子显示了如何将5维特征向量投影到3维主成分中。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">PCAExample</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder()</span><br><span class="line">      .appName(<span class="string">"PCAExample"</span>)</span><br><span class="line">      .master(<span class="string">"local"</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> data = <span class="type">Array</span>(</span><br><span class="line">      <span class="type">Vectors</span>.sparse(<span class="number">5</span>, <span class="type">Seq</span>((<span class="number">1</span>, <span class="number">1.0</span>), (<span class="number">3</span>, <span class="number">7.0</span>))),</span><br><span class="line">      <span class="type">Vectors</span>.dense(<span class="number">2.0</span>, <span class="number">0.0</span>, <span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>),</span><br><span class="line">      <span class="type">Vectors</span>.dense(<span class="number">4.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">6.0</span>, <span class="number">7.0</span>)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> df = spark.createDataFrame(data.map(<span class="type">Tuple1</span>.apply)).toDF(<span class="string">"features"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> pca = <span class="keyword">new</span> <span class="type">PCA</span>()</span><br><span class="line">      .setInputCol(<span class="string">"features"</span>)</span><br><span class="line">      .setOutputCol(<span class="string">"pcaFeatures"</span>)</span><br><span class="line">      .setK(<span class="number">3</span>)</span><br><span class="line">      .fit(df)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> result = pca.transform(df).select(<span class="string">"pcaFeatures"</span>)</span><br><span class="line">    result.show(<span class="literal">false</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>6.StringIndexer（字符串-索引变换）</strong></p>
<p>StringIndexer（字符串-索引变换）将标签的字符串列编号变成标签索引列。标签索引序列的取值范围是[0，numLabels（字符串中所有出现的单词去掉重复的词后的总和）]，按照标签出现频率排序，出现最多的标签索引为0。如果输入是数值型，我们先将数值映射到字符串，再对字符串进行索引化。如果下游的 pipeline（例如：Estimator 或者 Transformer）需要用到索引化后的标签序列，则需要将这个 pipeline 的输入列名字指定为索引化序列的名字。大部分情况下，通过 setInputCol 设置输入的列名。</p>
<p>假设我们有如下的 DataFrame ，包含有 id 和 category 两列、</p>
<table>
<thead>
<tr>
<th>id</th>
<th>category</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>a</td>
</tr>
<tr>
<td>1</td>
<td>b</td>
</tr>
<tr>
<td>2</td>
<td>c</td>
</tr>
<tr>
<td>3</td>
<td>a</td>
</tr>
<tr>
<td>4</td>
<td>a</td>
</tr>
<tr>
<td>5</td>
<td>c</td>
</tr>
</tbody>
</table>
<p>标签类别（category）是有3种取值的标签：“a”，“b”，“c”。使用 StringIndexer 通过 category 进行转换成 categoryIndex 后可以得到如下结果：</p>
<p>+—+——–+————-+<br>| id|category|categoryIndex|<br>+—+——–+————-+<br>|  0|       a|          0.0|<br>|  1|       b|          2.0|<br>|  2|       c|          1.0|<br>|  3|       a|          0.0|<br>|  4|       a|          0.0|<br>|  5|       c|          1.0|<br>+—+——–+————-+</p>
<p>“a”因为出现的次数最多，所以得到为0的索引（index）。第二多的“c”得到1的索引，“b”得到2的索引</p>
<p>另外，StringIndexer 在转换新数据时提供两种容错机制处理训练中没有出现的标签</p>
<ul>
<li>StringIndexer 抛出异常错误（默认值）</li>
<li>跳过未出现的标签实例。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> df = spark.createDataFrame(<span class="type">Seq</span>(</span><br><span class="line">      (<span class="number">0</span>, <span class="string">"a"</span>),</span><br><span class="line">      (<span class="number">1</span>, <span class="string">"b"</span>),</span><br><span class="line">      (<span class="number">2</span>, <span class="string">"c"</span>),</span><br><span class="line">      (<span class="number">3</span>, <span class="string">"a"</span>),</span><br><span class="line">      (<span class="number">4</span>, <span class="string">"a"</span>),</span><br><span class="line">      (<span class="number">5</span>, <span class="string">"c"</span>)</span><br><span class="line">    )).toDF(<span class="string">"id"</span>,<span class="string">"category"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> indexer = <span class="keyword">new</span> <span class="type">StringIndexer</span>()</span><br><span class="line">      .setInputCol(<span class="string">"category"</span>)</span><br><span class="line">      .setOutputCol(<span class="string">"categoryIndex"</span>)</span><br><span class="line">      .fit(df)</span><br><span class="line">    <span class="keyword">val</span> indexed = indexer.transform(df)</span><br><span class="line">    println(<span class="string">s"Transformed string column '<span class="subst">$&#123;indexer.getInputCol&#125;</span>' "</span> +</span><br><span class="line">      <span class="string">s"to indexed column '<span class="subst">$&#123;indexer.getOutputCol&#125;</span>'"</span>)</span><br><span class="line">    indexed.show()</span><br></pre></td></tr></table></figure>
<p><strong>7.OneHot独热编码</strong></p>
<p><a href="http://en.wikipedia.org/wiki/One-hot" target="_blank" rel="noopener">独热编码（One-hot encoding）</a>将一列标签索引映射到一列二进制向量，最多只有一个单值。 该编码允许期望连续特征（例如逻辑回归）的算法使用分类特征。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">OneHotExample</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder()</span><br><span class="line">      .appName(<span class="string">"BinaryExample"</span>)</span><br><span class="line">      .master(<span class="string">"local"</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> df = spark.createDataFrame(<span class="type">Seq</span>(</span><br><span class="line">      (<span class="number">0</span>, <span class="string">"a"</span>),</span><br><span class="line">      (<span class="number">1</span>, <span class="string">"b"</span>),</span><br><span class="line">      (<span class="number">2</span>, <span class="string">"c"</span>),</span><br><span class="line">      (<span class="number">3</span>, <span class="string">"a"</span>),</span><br><span class="line">      (<span class="number">4</span>, <span class="string">"a"</span>),</span><br><span class="line">      (<span class="number">5</span>, <span class="string">"c"</span>)</span><br><span class="line">    )).toDF(<span class="string">"id"</span>,<span class="string">"category"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> indexer = <span class="keyword">new</span> <span class="type">StringIndexer</span>()</span><br><span class="line">      .setInputCol(<span class="string">"category"</span>)</span><br><span class="line">      .setOutputCol(<span class="string">"categoryIndex"</span>)</span><br><span class="line">      .fit(df)</span><br><span class="line">    <span class="keyword">val</span> indexed = indexer.transform(df)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> encoder = <span class="keyword">new</span> <span class="type">OneHotEncoder</span>()</span><br><span class="line">      .setInputCol(<span class="string">"categoryIndex"</span>)</span><br><span class="line">      .setOutputCol(<span class="string">"categoryVec"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> encoded = encoder.transform(indexed)</span><br><span class="line">    encoded.show(<span class="literal">false</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>8.VectorIndexer(向量类型索引化)</strong></p>
<p>VectorIndexer可以帮助指定向量数据集中的分类特征。它可以自动确定哪些功能是分类的，并将原始值转换为类别索引。具体来说，它执行以下操作：</p>
<ol>
<li>取一个Vector类型的输入列和一个参数maxCategories。</li>
<li>根据不同值的数量确定哪些功能应分类，其中最多maxCategories的功能被声明为分类。</li>
<li>为每个分类功能计算基于0的类别索引。</li>
<li>索引分类特征并将原始特征值转换为索引。</li>
</ol>
<p>索引分类功能允许诸如决策树和树组合之类的算法适当地处理分类特征，提高性能。</p>
<p>在下面的示例中，我们读取标注点的数据集，然后使用VectorIndexer来确定哪些功能应被视为分类。我们将分类特征值转换为其索引。然后，该转换的数据可以传递给诸如DecisionTreeRegressor之类的算法来处理分类特征。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">VectorIndexerExample</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder()</span><br><span class="line">      .appName(<span class="string">"VectorIndexerExample"</span>)</span><br><span class="line">      .master(<span class="string">"local"</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> data = spark.read.format(<span class="string">"libsvm"</span>).load(<span class="string">"data/mllib/sample_libsvm_data.txt"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> indexer = <span class="keyword">new</span> <span class="type">VectorIndexer</span>()</span><br><span class="line">      .setInputCol(<span class="string">"features"</span>)</span><br><span class="line">      .setOutputCol(<span class="string">"indexed"</span>)</span><br><span class="line">      .setMaxCategories(<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> indexerModel = indexer.fit(data)</span><br><span class="line">    <span class="keyword">val</span> categoricalFeatures : <span class="type">Set</span>[<span class="type">Int</span>] = indexerModel.categoryMaps.keys.toSet</span><br><span class="line">    println(<span class="string">s"Choose <span class="subst">$&#123;categoricalFeatures.size&#125;</span> categorical features: "</span> + categoricalFeatures.mkString(<span class="string">", "</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> indexedData = indexerModel.transform(data)</span><br><span class="line">    indexedData.show()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>9.Interaction相互作用</strong></p>
<p>交互是一个变换器，它采用向量或双值列，并生成一个单个向量列，其中包含来自每个输入列的一个值的所有组合的乘积。</p>
<p>例如，如果有2个向量类型的列，每个列具有3个维度作为输入列，那么将获得一个9维向量作为输出列。</p>
<p> 假设我们有如下DataFrame，列为“id1”, “vec1” 和 “vec2”:</p>
<table>
<thead>
<tr>
<th>id1</th>
<th>vec1</th>
<th>vec2</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1, 2, 3</td>
<td>8, 4, 5</td>
</tr>
<tr>
<td>2</td>
<td>4, 3, 8</td>
<td>7, 9, 8</td>
</tr>
</tbody>
</table>
<p>应用与这些输入列的交互，然后将交互作为输出列包含：</p>
<table>
<thead>
<tr>
<th>id1</th>
<th>vec1</th>
<th>vec2</th>
<th>interactedCol</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1, 2, 3</td>
<td>8, 4, 5</td>
<td>8, 4, 5, 16, 8, 10, 24, 12, 15</td>
</tr>
<tr>
<td>2</td>
<td>4, 3, 8</td>
<td>7, 9, 8</td>
<td>56,  72,  64, 42, 54, 48, 112, 144, 128</td>
</tr>
</tbody>
</table>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">InteractionExample</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder()</span><br><span class="line">      .appName(<span class="string">"VectorIndexerExample"</span>)</span><br><span class="line">      .master(<span class="string">"local"</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> df = spark.createDataFrame(<span class="type">Seq</span>(</span><br><span class="line">      (<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">8</span>, <span class="number">4</span>, <span class="number">5</span>),</span><br><span class="line">      (<span class="number">2</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">8</span>, <span class="number">7</span>, <span class="number">9</span>, <span class="number">8</span>),</span><br><span class="line">      (<span class="number">3</span>, <span class="number">6</span>, <span class="number">1</span>, <span class="number">9</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">6</span>),</span><br><span class="line">      (<span class="number">4</span>, <span class="number">10</span>, <span class="number">8</span>, <span class="number">6</span>, <span class="number">9</span>, <span class="number">4</span>, <span class="number">5</span>),</span><br><span class="line">      (<span class="number">5</span>, <span class="number">9</span>, <span class="number">2</span>, <span class="number">7</span>, <span class="number">10</span>, <span class="number">7</span>, <span class="number">3</span>),</span><br><span class="line">      (<span class="number">6</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">8</span>, <span class="number">4</span>)</span><br><span class="line">    )).toDF(<span class="string">"id1"</span>, <span class="string">"id2"</span>, <span class="string">"id3"</span>, <span class="string">"id4"</span>, <span class="string">"id5"</span>, <span class="string">"id6"</span>, <span class="string">"id7"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> assembler1 = <span class="keyword">new</span> <span class="type">VectorAssembler</span>()</span><br><span class="line">      .setInputCols(<span class="type">Array</span>(<span class="string">"id2"</span>,<span class="string">"id2"</span>,<span class="string">"id4"</span>))</span><br><span class="line">      .setOutputCol(<span class="string">"vec1"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> assembled1 = assembler1.transform(df)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> assembler2 = <span class="keyword">new</span> <span class="type">VectorAssembler</span>().</span><br><span class="line">      setInputCols(<span class="type">Array</span>(<span class="string">"id5"</span>, <span class="string">"id6"</span>, <span class="string">"id7"</span>)).</span><br><span class="line">      setOutputCol(<span class="string">"vec2"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> assembled2 = assembler2.transform(assembled1).select(<span class="string">"id1"</span>,<span class="string">"vec1"</span>,<span class="string">"vec2"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> interaction = <span class="keyword">new</span> <span class="type">Interaction</span>()</span><br><span class="line">      .setInputCols(<span class="type">Array</span>(<span class="string">"id1"</span>, <span class="string">"vec1"</span>, <span class="string">"vec2"</span>))</span><br><span class="line">        .setOutputCol(<span class="string">"interactedCol"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> interacted = interaction.transform(assembled2)</span><br><span class="line"></span><br><span class="line">    interacted.show(<span class="literal">false</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>10.Normalizer(范数p-norm规范化)</strong></p>
<p>Normalizer是一个转换器，它可以将一组特征向量（通过计算p-范数）规范化。参数为p（默认值：2）来指定规范化中使用的p-norm。规范化操作可以使输入数据标准化，对后期机器学习算法的结果也有更好的表现。</p>
<p>下面的例子展示如何读入一个libsvm格式的数据，然后将每一行转换为$L^2$以及$L^\infty$形式。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">NormalizerExample</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder()</span><br><span class="line">      .appName(<span class="string">"NormalizerExample"</span>)</span><br><span class="line">      .master(<span class="string">"local"</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> dataFrame = spark.createDataFrame(<span class="type">Seq</span>(</span><br><span class="line">      (<span class="number">0</span>, <span class="type">Vectors</span>.dense(<span class="number">1.0</span>, <span class="number">0.5</span>, <span class="number">-1.0</span>)),</span><br><span class="line">      (<span class="number">1</span>, <span class="type">Vectors</span>.dense(<span class="number">2.0</span>, <span class="number">1.0</span>, <span class="number">1.0</span>)),</span><br><span class="line">      (<span class="number">2</span>, <span class="type">Vectors</span>.dense(<span class="number">4.0</span>, <span class="number">10.0</span>, <span class="number">2.0</span>))</span><br><span class="line">    )).toDF(<span class="string">"id"</span>, <span class="string">"features"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> normalizer = <span class="keyword">new</span> <span class="type">Normalizer</span>()</span><br><span class="line">      .setInputCol(<span class="string">"features"</span>)</span><br><span class="line">      .setOutputCol(<span class="string">"normFeatures"</span>)</span><br><span class="line">      .setP(<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> l1NormData = normalizer.transform(dataFrame)</span><br><span class="line">    println(<span class="string">"Normalized using L^1 norm"</span>)</span><br><span class="line">    l1NormData.show()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> lInfNormData = normalizer.transform(dataFrame, normalizer.p -&gt; <span class="type">Double</span>.<span class="type">PositiveInfinity</span>)</span><br><span class="line">    println(<span class="string">"Normalized using L^inf norm"</span>)</span><br><span class="line">    lInfNormData.show()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>11.StandardScalaer标准化</strong></p>
<p>StandardScaler转换Vector行的数据集，使每个要素标准化以具有单位标准偏差和 或 零均值。它需要参数：</p>
<ul>
<li>withStd：默认为True。将数据缩放到单位标准偏差。</li>
<li>withMean：默认为false。在缩放之前将数据中心为平均值。它将构建一个密集的输出，所以在应用于稀疏输入时要小心。</li>
</ul>
<p>StandardScaler是一个Estimator，可以适合数据集生成StandardScalerModel; 这相当于计算汇总统计数据。 然后，模型可以将数据集中的向量列转换为具有单位标准偏差和/或零平均特征。</p>
<p>请注意，如果特征的标准偏差为零，它将在该特征的向量中返回默认的0.0值。</p>
<p>以下示例演示如何以libsvm格式加载数据集，然后将每个要素归一化以具有单位标准偏差。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StandardScalerExample</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder()</span><br><span class="line">      .appName(<span class="string">"NormalizerExample"</span>)</span><br><span class="line">      .master(<span class="string">"local"</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> dataFrame = spark.read.format(<span class="string">"libsvm"</span>).load(<span class="string">"data/mllib/sample_libsvm_data.txt"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> scaler = <span class="keyword">new</span> <span class="type">StandardScaler</span>()</span><br><span class="line">      .setInputCol(<span class="string">"features"</span>)</span><br><span class="line">      .setOutputCol(<span class="string">"scalerFeatures"</span>)</span><br><span class="line">      .setWithStd(<span class="literal">true</span>)</span><br><span class="line">      .setWithMean(<span class="literal">false</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> scalerModel = scaler.fit(dataFrame)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> scaledData = scalerModel.transform(dataFrame)</span><br><span class="line">    scaledData.show()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>12.MinMaxScaler最大最小规范化</strong></p>
<p>MinMaxScaler转换Vector行的数据集，将每个要素的重新映射到特定范围（通常为[0，1]）。它需要参数：</p>
<ul>
<li>min：默认为0.0，转换后的下限，由所有功能共享。</li>
<li>max：默认为1.0，转换后的上限，由所有功能共享。</li>
</ul>
<p>MinMaxScaler计算数据集的统计信息，并生成MinMaxScalerModel。然后，模型可以单独转换每个要素，使其在给定的范围内。</p>
<p>特征E的重新缩放值被计算为：</p>
<p>​                           $Rescaleed(e_i) = \frac {e_i - E_min}{E_max - E_min}*(max-min) + min$</p>
<p>对于$E_{max}  = E_{min},Rescaled(e_i) = 0.5*(max+min)$</p>
<p>请注意，由于零值可能会转换为非零值，即使对于稀疏输入，变压器的输出也将为DenseVector。</p>
<p>以下示例演示如何以libsvm格式加载数据集，然后将每个要素重新缩放为[0，1]。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MinMaxScalerExample</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder()</span><br><span class="line">      .appName(<span class="string">"NormalizerExample"</span>)</span><br><span class="line">      .master(<span class="string">"local"</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> dataFrame = spark.createDataFrame(<span class="type">Seq</span>(</span><br><span class="line">      (<span class="number">0</span>, <span class="type">Vectors</span>.dense(<span class="number">1.0</span>,<span class="number">0.1</span>,<span class="number">-1.0</span>)),</span><br><span class="line">      (<span class="number">1</span>, <span class="type">Vectors</span>.dense(<span class="number">2.0</span>, <span class="number">1.1</span>, <span class="number">1.0</span>)),</span><br><span class="line">      (<span class="number">2</span>, <span class="type">Vectors</span>.dense(<span class="number">3.0</span>, <span class="number">10.1</span>, <span class="number">3.0</span>))</span><br><span class="line">    )).toDF(<span class="string">"id"</span>, <span class="string">"features"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> scaler = <span class="keyword">new</span> <span class="type">MinMaxScaler</span>()</span><br><span class="line">      .setInputCol(<span class="string">"features"</span>)</span><br><span class="line">      .setOutputCol(<span class="string">"scaledFeatures"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> scalerModel = scaler.fit(dataFrame)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> scaledData = scalerModel.transform(dataFrame)</span><br><span class="line">    println(<span class="string">s"Features scaled to range [<span class="subst">$&#123;scaler.getMin&#125;</span>, <span class="subst">$&#123;scaler.getMax&#125;</span>]"</span>)</span><br><span class="line">    scaledData.select(<span class="string">"features"</span>,<span class="string">"scaledFeatures"</span>).show()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>13.Bucketizer分箱器</strong></p>
<p><strong>Bucketizer</strong> 将一列连续的特征转换为特征 <strong>buckets</strong>（区间），<strong>buckets</strong>（区间）由用户指定。<strong>Bucketizer</strong> 需要一个参数：</p>
<p>splits（分割）：这是个将连续的特征转换为 <strong>buckets</strong>（区间）的参数. <strong>n+1</strong>次分割时，将产生n个 <strong>buckets（</strong>区间）。一个<strong>bucket（</strong>区间）通过范围 <strong>[x,y)</strong> 中 <strong>x</strong> , <strong>y</strong> 来定义除了最后一个 <strong>bucket</strong> 包含 <strong>y</strong> 值。Splits（分割）应该是严格递增的。<strong>-inf, inf</strong> 之间的值必须明确提供来覆盖所有的 <strong>Double</strong> 值;另外,<strong>Double</strong> 值超出 <strong>splits</strong>（分割）指定的值将认为是错误的. 两个<strong>splits</strong> （拆分）的例子为 Array(Double.NegativeInfinity, 0.0, 1.0, Double.PositiveInfinity)以及Array(0.0, 1.0, 2.0)。</p>
<p>请注意,如果你不知道目标列的上线和下限,则应将 <strong>Double.NegativeInfinity</strong> 和  <strong>Double.PositiveInfinity</strong> 添加为<strong>splits</strong>（分割）的边界,以防止 <strong>Bucketizer</strong> 界限出现异常.</p>
<p>还请注意,提供的 <strong>splits</strong>（分割）必须严格按照增加的顺序,即 <strong>s0 &lt; s1 &lt; s2 &lt; … &lt; sn.</strong></p>
<p>下面这个例子演示了如何将包含 <strong>Doubles 的一列 bucketize</strong> （分箱）为另外一个索引列.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">BucketizerExample</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder()</span><br><span class="line">      .master(<span class="string">"local"</span>)</span><br><span class="line">      .appName(<span class="string">"BucketizerExample"</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> splits = <span class="type">Array</span>(<span class="type">Double</span>.<span class="type">NegativeInfinity</span>, <span class="number">-0.5</span>, <span class="number">0.0</span>, <span class="number">0.5</span>, <span class="type">Double</span>.<span class="type">PositiveInfinity</span>)</span><br><span class="line">    <span class="keyword">val</span> data = <span class="type">Array</span>(<span class="number">-0.5</span>,<span class="number">-0.3</span>,<span class="number">0.0</span>,<span class="number">0.2</span>,<span class="number">0.8</span>)</span><br><span class="line">    <span class="keyword">val</span> dataFrame = spark.createDataFrame(data.map(<span class="type">Tuple1</span>.apply)).toDF(<span class="string">"features"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> bucketizer = <span class="keyword">new</span> <span class="type">Bucketizer</span>()</span><br><span class="line">      .setInputCol(<span class="string">"features"</span>)</span><br><span class="line">      .setOutputCol(<span class="string">"bucketedFeatures"</span>)</span><br><span class="line">      .setSplits(splits)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> bucketedData = bucketizer.transform(dataFrame)</span><br><span class="line">    bucketedData.show()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
  </section>
</article>

  <div class="sharing grid">
  <section class="profile grid-item grid">
    <img class="avatar" src="http://7xrcp8.com1.z0.glb.clouddn.com/avatar.png" alt="avatar" />
    <div class="grid-item">
      <p class="title"> lyyourc </p>
      <p class="subtitle"> You Are The JavaScript In My HTML </p>
    <div>
  </section>

  <section class="share-btns">
    <!-- <p> share it if you like it~ </p> -->
    <a
  class="twitter-share-button"
  data-size="large"
  data-via="DrakeLeung"
  href="https://twitter.com/intent/tweet?text=本博客介绍使用功能的算法，大致分为以下几"
>
  Tweet
</a>

<script>
  window.twttr = (function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0],
    t = window.twttr || {};
  if (d.getElementById(id)) return t;
  js = d.createElement(s);
  js.id = id;
  js.src = "https://platform.twitter.com/widgets.js";
  js.async = true;
  fjs.parentNode.insertBefore(js, fjs);

  t._e = [];
  t.ready = function(f) {
    t._e.push(f);
  };

  return t;
}(document, "script", "twitter-wjs"));
</script>

  </section>
</div>


  
    
<section class="article-comment">
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
</section>

<script>
  var disqus_shortname = 'drakeleung';
  
  var disqus_url = '//harold.me/2018/06/26/MLlib-特征的提取、转换与选择/';
  
  (function(){
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>


  
</main>

</body>
</html>
