<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>lightGbm简要原理 | Harold的博客</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="本文转自:https://blog.csdn.net/niaolianjiulin/article/details/76584785 和https://www.cnblogs.com/wanglei5205/p/8722237.html 1. lightGBM简介xgboost的出现，让数据民工们告别了传统的机器学习算法们：RF、GBM、SVM、LASSO……..。现在微软推出了一个新的boost">
<meta name="keywords" content="机器学习">
<meta property="og:type" content="article">
<meta property="og:title" content="lightGbm简要原理">
<meta property="og:url" content="//harold.me/2018/06/07/lightGbm简要原理/index.html">
<meta property="og:site_name" content="Harold的博客">
<meta property="og:description" content="本文转自:https://blog.csdn.net/niaolianjiulin/article/details/76584785 和https://www.cnblogs.com/wanglei5205/p/8722237.html 1. lightGBM简介xgboost的出现，让数据民工们告别了传统的机器学习算法们：RF、GBM、SVM、LASSO……..。现在微软推出了一个新的boost">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://img-blog.csdn.net/20170802163943148?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbmlhb2xpYW5qaXVsaW4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">
<meta property="og:image" content="https://images2018.cnblogs.com/blog/1307402/201804/1307402-20180405133821380-233032611.png">
<meta property="og:image" content="https://common.cnblogs.com/images/copycode.gif">
<meta property="og:updated_time" content="2018-06-07T14:06:34.267Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="lightGbm简要原理">
<meta name="twitter:description" content="本文转自:https://blog.csdn.net/niaolianjiulin/article/details/76584785 和https://www.cnblogs.com/wanglei5205/p/8722237.html 1. lightGBM简介xgboost的出现，让数据民工们告别了传统的机器学习算法们：RF、GBM、SVM、LASSO……..。现在微软推出了一个新的boost">
<meta name="twitter:image" content="https://img-blog.csdn.net/20170802163943148?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbmlhb2xpYW5qaXVsaW4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">
  
    <link rel="alternate" href="/atom.xml" title="Harold的博客" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Harold的博客</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">踏实前进,戒骄戒躁</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="//harold.me"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-lightGbm简要原理" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/06/07/lightGbm简要原理/" class="article-date">
  <time datetime="2018-06-06T17:47:18.000Z" itemprop="datePublished">2018-06-07</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      lightGbm简要原理
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>本文转自:<a href="https://blog.csdn.net/niaolianjiulin/article/details/76584785" target="_blank" rel="noopener">https://blog.csdn.net/niaolianjiulin/article/details/76584785</a></p>
<p>和<a href="https://www.cnblogs.com/wanglei5205/p/8722237.html" target="_blank" rel="noopener">https://www.cnblogs.com/wanglei5205/p/8722237.html</a></p>
<h1 id="1-lightGBM简介"><a href="#1-lightGBM简介" class="headerlink" title="1. lightGBM简介"></a>1. lightGBM简介</h1><p>xgboost的出现，让数据民工们告别了传统的机器学习算法们：RF、GBM、SVM、LASSO……..。现在微软推出了一个新的boosting框架，想要挑战xgboost的江湖地位。</p>
<p>顾名思义，lightGBM包含两个关键点：light即轻量级，GBM 梯度提升机。</p>
<p>LightGBM 是一个梯度 boosting 框架，使用基于学习算法的决策树。它可以说是分布式的，高效的，有以下优势：<br> <a id="more"></a></p>
<ul>
<li>更快的训练效率</li>
<li>低内存使用</li>
<li>更高的准确率</li>
<li>支持并行化学习</li>
<li>可处理大规模数据</li>
</ul>
<p>与常用的机器学习算法进行比较：速度飞起</p>
<p><img src="https://img-blog.csdn.net/20170802163943148?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbmlhb2xpYW5qaXVsaW4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p>
<h1 id="2-xgboost缺点"><a href="#2-xgboost缺点" class="headerlink" title="2. xgboost缺点"></a>2. xgboost缺点</h1><p>XGB的介绍见<a href="http://blog.csdn.net/niaolianjiulin/article/details/76574216" target="_blank" rel="noopener">此篇博文</a></p>
<p>其缺点，或者说不足之处：</p>
<ul>
<li>每轮迭代时，都需要遍历整个训练数据多次。如果把整个训练数据装进内存则会限制训练数据的大小；如果不装进内存，反复地读写训练数据又会消耗非常大的时间。</li>
<li>预排序方法（pre-sorted）：首先，空间消耗大。这样的算法需要保存数据的特征值，还保存了特征排序的结果（例如排序后的索引，为了后续快速的计算分割点），这里需要消耗训练数据两倍的内存。其次时间上也有较大的开销，在遍历每一个分割点的时候，都需要进行分裂增益的计算，消耗的代价大。</li>
<li>对cache优化不友好。在预排序后，特征对梯度的访问是一种随机访问，并且不同的特征访问的顺序不一样，无法对cache进行优化。同时，在每一层长树的时候，需要随机访问一个行索引到叶子索引的数组，并且不同特征访问的顺序也不一样，也会造成较大的cache miss。</li>
</ul>
<h1 id="3-lightGBM特点"><a href="#3-lightGBM特点" class="headerlink" title="3. lightGBM特点"></a>3. lightGBM特点</h1><p>以上与其说是xgboost的不足，倒不如说是lightGBM作者们构建新算法时着重瞄准的点。解决了什么问题，那么原来模型没解决就成了原模型的缺点。</p>
<p>概括来说，lightGBM主要有以下特点：</p>
<ul>
<li><strong>基于Histogram的决策树算法</strong></li>
<li><strong>带深度限制的Leaf-wise的叶子生长策略</strong></li>
<li>直方图做差加速</li>
<li>直接支持类别特征(Categorical Feature)</li>
<li>Cache命中率优化</li>
<li>基于直方图的稀疏特征优化</li>
<li>多线程优化</li>
</ul>
<p>前2个特点使我们尤为关注的。</p>
<p><strong>Histogram算法</strong></p>
<p>直方图算法的基本思想：先把连续的浮点特征值离散化成k个整数，同时构造一个宽度为k的直方图。遍历数据时，根据离散化后的值作为索引在直方图中累积统计量，当遍历一次数据后，直方图累积了需要的统计量，然后根据直方图的离散值，遍历寻找最优的分割点。</p>
<p><strong>带深度限制的Leaf-wise的叶子生长策略</strong></p>
<p>Level-wise过一次数据可以同时分裂同一层的叶子，容易进行多线程优化，也好控制模型复杂度，不容易过拟合。但实际上Level-wise是一种低效算法，因为它不加区分的对待同一层的叶子，带来了很多没必要的开销，因为实际上很多叶子的分裂增益较低，没必要进行搜索和分裂。</p>
<p>Leaf-wise则是一种更为高效的策略：每次从当前所有叶子中，找到分裂增益最大的一个叶子，然后分裂，如此循环。因此同Level-wise相比，在分裂次数相同的情况下，Leaf-wise可以降低更多的误差，得到更好的精度。</p>
<p>Leaf-wise的缺点：可能会长出比较深的决策树，产生过拟合。因此LightGBM在Leaf-wise之上增加了一个最大深度限制，在保证高效率的同时防止过拟合。</p>
<h1 id="4-lightGBM调参"><a href="#4-lightGBM调参" class="headerlink" title="4. lightGBM调参"></a>4. lightGBM调参</h1><p>（1）num_leaves</p>
<p>LightGBM使用的是leaf-wise的算法，因此在调节树的复杂程度时，使用的是num_leaves而不是max_depth。</p>
<p>大致换算关系：num_leaves = 2^(max_depth)</p>
<p>（2）样本分布非平衡数据集：可以param[‘is_unbalance’]=’true’</p>
<p>（3）Bagging参数：bagging_fraction+bagging_freq（必须同时设置）、feature_fraction</p>
<p>（4）min_data_in_leaf、min_sum_hessian_in_leaf</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">// <span class="number">01.</span> train set <span class="keyword">and</span> test set</span><br><span class="line">train_data = lgb.Dataset(dtrain[predictors],label=dtrain[target],feature_name=list(dtrain[predictors].columns), categorical_feature=dummies)</span><br><span class="line"></span><br><span class="line">test_data = lgb.Dataset(dtest[predictors],label=dtest[target],feature_name=list(dtest[predictors].columns), categorical_feature=dummies)</span><br><span class="line"></span><br><span class="line">// <span class="number">02.</span> parameters</span><br><span class="line">param = &#123;</span><br><span class="line">    <span class="string">'max_depth'</span>:<span class="number">6</span>,</span><br><span class="line">    <span class="string">'num_leaves'</span>:<span class="number">64</span>,</span><br><span class="line">    <span class="string">'learning_rate'</span>:<span class="number">0.03</span>,</span><br><span class="line">    <span class="string">'scale_pos_weight'</span>:<span class="number">1</span>,</span><br><span class="line">    <span class="string">'num_threads'</span>:<span class="number">40</span>,</span><br><span class="line">    <span class="string">'objective'</span>:<span class="string">'binary'</span>,</span><br><span class="line">    <span class="string">'bagging_fraction'</span>:<span class="number">0.7</span>,</span><br><span class="line">    <span class="string">'bagging_freq'</span>:<span class="number">1</span>,</span><br><span class="line">    <span class="string">'min_sum_hessian_in_leaf'</span>:<span class="number">100</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">param[<span class="string">'is_unbalance'</span>]=<span class="string">'true'</span></span><br><span class="line">param[<span class="string">'metric'</span>] = <span class="string">'auc'</span></span><br><span class="line"></span><br><span class="line">// <span class="number">03.</span> cv <span class="keyword">and</span> train</span><br><span class="line">bst=lgb.cv(param,train_data, num_boost_round=<span class="number">1000</span>, nfold=<span class="number">3</span>, early_stopping_rounds=<span class="number">30</span>)</span><br><span class="line"></span><br><span class="line">estimators = lgb.train(param,train_data,num_boost_round=len(bst[<span class="string">'auc-mean'</span>]))</span><br><span class="line"></span><br><span class="line">// <span class="number">04.</span> test predict</span><br><span class="line">ypred = estimators.predict(dtest[predictors])<span class="number">12345678910111213141516171819202122232425262728</span></span><br></pre></td></tr></table></figure>
<p><strong># lightgbm关键参数</strong></p>
<p><a href="https://images2018.cnblogs.com/blog/1307402/201804/1307402-20180405133820459-1387924067.png" target="_blank" rel="noopener"><img src="https://images2018.cnblogs.com/blog/1307402/201804/1307402-20180405133821380-233032611.png" alt="image"></a></p>
<p><strong># lightgbm调参方法cv</strong></p>
<p><a href="https://github.com/wanglei5205/Machine_learning/blob/master/Boosting--LightGBM/lgb-python/2.lightgbm%E8%B0%83%E5%8F%82%E6%A1%88%E4%BE%8B.py" target="_blank" rel="noopener">代码github地址</a></p>
<p><a href="javascript:void(0" target="_blank" rel="noopener"><img src="https://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a>;)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br></pre></td><td class="code"><pre><span class="line">  <span class="number">1</span> <span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line">  <span class="number">2</span> <span class="string">"""</span></span><br><span class="line"><span class="string">  3 # 作者：wanglei5205</span></span><br><span class="line"><span class="string">  4 # 邮箱：wanglei5205@126.com</span></span><br><span class="line"><span class="string">  5 # 博客：http://cnblogs.com/wanglei5205</span></span><br><span class="line"><span class="string">  6 # github：http://github.com/wanglei5205</span></span><br><span class="line"><span class="string">  7 """</span></span><br><span class="line">  <span class="number">8</span> <span class="comment">### 导入模块</span></span><br><span class="line">  <span class="number">9</span> <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"> <span class="number">10</span> <span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"> <span class="number">11</span> <span class="keyword">import</span> lightgbm <span class="keyword">as</span> lgb</span><br><span class="line"> <span class="number">12</span> <span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"> <span class="number">13</span> </span><br><span class="line"> <span class="number">14</span> <span class="comment">### 载入数据</span></span><br><span class="line"> <span class="number">15</span> print(<span class="string">'载入数据'</span>)</span><br><span class="line"> <span class="number">16</span> dataset1 = pd.read_csv(<span class="string">'G:/ML/ML_match/IJCAI/data3.22/3.22ICJAI/data/7_train_data1.csv'</span>)</span><br><span class="line"> <span class="number">17</span> dataset2 = pd.read_csv(<span class="string">'G:/ML/ML_match/IJCAI/data3.22/3.22ICJAI/data/7_train_data2.csv'</span>)</span><br><span class="line"> <span class="number">18</span> dataset3 = pd.read_csv(<span class="string">'G:/ML/ML_match/IJCAI/data3.22/3.22ICJAI/data/7_train_data3.csv'</span>)</span><br><span class="line"> <span class="number">19</span> dataset4 = pd.read_csv(<span class="string">'G:/ML/ML_match/IJCAI/data3.22/3.22ICJAI/data/7_train_data4.csv'</span>)</span><br><span class="line"> <span class="number">20</span> dataset5 = pd.read_csv(<span class="string">'G:/ML/ML_match/IJCAI/data3.22/3.22ICJAI/data/7_train_data5.csv'</span>)</span><br><span class="line"> <span class="number">21</span> </span><br><span class="line"> <span class="number">22</span> print(<span class="string">'数据去重'</span>)</span><br><span class="line"> <span class="number">23</span> dataset1.drop_duplicates(inplace=<span class="keyword">True</span>)</span><br><span class="line"> <span class="number">24</span> dataset2.drop_duplicates(inplace=<span class="keyword">True</span>)</span><br><span class="line"> <span class="number">25</span> dataset3.drop_duplicates(inplace=<span class="keyword">True</span>)</span><br><span class="line"> <span class="number">26</span> dataset4.drop_duplicates(inplace=<span class="keyword">True</span>)</span><br><span class="line"> <span class="number">27</span> dataset5.drop_duplicates(inplace=<span class="keyword">True</span>)</span><br><span class="line"> <span class="number">28</span> </span><br><span class="line"> <span class="number">29</span> print(<span class="string">'数据合并'</span>)</span><br><span class="line"> <span class="number">30</span> trains = pd.concat([dataset1,dataset2],axis=<span class="number">0</span>)</span><br><span class="line"> <span class="number">31</span> trains = pd.concat([trains,dataset3],axis=<span class="number">0</span>)</span><br><span class="line"> <span class="number">32</span> trains = pd.concat([trains,dataset4],axis=<span class="number">0</span>)</span><br><span class="line"> <span class="number">33</span> </span><br><span class="line"> <span class="number">34</span> online_test = dataset5</span><br><span class="line"> <span class="number">35</span> </span><br><span class="line"> <span class="number">36</span> <span class="comment">### 数据拆分(训练集+验证集+测试集)</span></span><br><span class="line"> <span class="number">37</span> print(<span class="string">'数据拆分'</span>)</span><br><span class="line"> <span class="number">38</span> <span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"> <span class="number">39</span> train_xy,offline_test = train_test_split(trains,test_size = <span class="number">0.2</span>,random_state=<span class="number">21</span>)</span><br><span class="line"> <span class="number">40</span> train,val = train_test_split(train_xy,test_size = <span class="number">0.2</span>,random_state=<span class="number">21</span>)</span><br><span class="line"> <span class="number">41</span> </span><br><span class="line"> <span class="number">42</span> <span class="comment"># 训练集</span></span><br><span class="line"> <span class="number">43</span> y_train = train.is_trade                                               <span class="comment"># 训练集标签</span></span><br><span class="line"> <span class="number">44</span> X_train = train.drop([<span class="string">'instance_id'</span>,<span class="string">'is_trade'</span>],axis=<span class="number">1</span>)                <span class="comment"># 训练集特征矩阵</span></span><br><span class="line"> <span class="number">45</span> </span><br><span class="line"> <span class="number">46</span> <span class="comment"># 验证集</span></span><br><span class="line"> <span class="number">47</span> y_val = val.is_trade                                                   <span class="comment"># 验证集标签</span></span><br><span class="line"> <span class="number">48</span> X_val = val.drop([<span class="string">'instance_id'</span>,<span class="string">'is_trade'</span>],axis=<span class="number">1</span>)                    <span class="comment"># 验证集特征矩阵</span></span><br><span class="line"> <span class="number">49</span> </span><br><span class="line"> <span class="number">50</span> <span class="comment"># 测试集</span></span><br><span class="line"> <span class="number">51</span> offline_test_X = offline_test.drop([<span class="string">'instance_id'</span>,<span class="string">'is_trade'</span>],axis=<span class="number">1</span>)  <span class="comment"># 线下测试特征矩阵</span></span><br><span class="line"> <span class="number">52</span> online_test_X  = online_test.drop([<span class="string">'instance_id'</span>],axis=<span class="number">1</span>)              <span class="comment"># 线上测试特征矩阵</span></span><br><span class="line"> <span class="number">53</span> </span><br><span class="line"> <span class="number">54</span> <span class="comment">### 数据转换</span></span><br><span class="line"> <span class="number">55</span> print(<span class="string">'数据转换'</span>)</span><br><span class="line"> <span class="number">56</span> lgb_train = lgb.Dataset(X_train, y_train, free_raw_data=<span class="keyword">False</span>)</span><br><span class="line"> <span class="number">57</span> lgb_eval = lgb.Dataset(X_val, y_val, reference=lgb_train,free_raw_data=<span class="keyword">False</span>)</span><br><span class="line"> <span class="number">58</span> </span><br><span class="line"> <span class="number">59</span> <span class="comment">### 设置初始参数--不含交叉验证参数</span></span><br><span class="line"> <span class="number">60</span> print(<span class="string">'设置参数'</span>)</span><br><span class="line"> <span class="number">61</span> params = &#123;</span><br><span class="line"> <span class="number">62</span>           <span class="string">'boosting_type'</span>: <span class="string">'gbdt'</span>,</span><br><span class="line"> <span class="number">63</span>           <span class="string">'objective'</span>: <span class="string">'binary'</span>,</span><br><span class="line"> <span class="number">64</span>           <span class="string">'metric'</span>: <span class="string">'binary_logloss'</span>,</span><br><span class="line"> <span class="number">65</span>           &#125;</span><br><span class="line"> <span class="number">66</span> </span><br><span class="line"> <span class="number">67</span> <span class="comment">### 交叉验证(调参)</span></span><br><span class="line"> <span class="number">68</span> print(<span class="string">'交叉验证'</span>)</span><br><span class="line"> <span class="number">69</span> min_merror = float(<span class="string">'Inf'</span>)</span><br><span class="line"> <span class="number">70</span> best_params = &#123;&#125;</span><br><span class="line"> <span class="number">71</span> </span><br><span class="line"> <span class="number">72</span> <span class="comment"># 准确率</span></span><br><span class="line"> <span class="number">73</span> print(<span class="string">"调参1：提高准确率"</span>)</span><br><span class="line"> <span class="number">74</span> <span class="keyword">for</span> num_leaves <span class="keyword">in</span> range(<span class="number">20</span>,<span class="number">200</span>,<span class="number">5</span>):</span><br><span class="line"> <span class="number">75</span>     <span class="keyword">for</span> max_depth <span class="keyword">in</span> range(<span class="number">3</span>,<span class="number">8</span>,<span class="number">1</span>):</span><br><span class="line"> <span class="number">76</span>         params[<span class="string">'num_leaves'</span>] = num_leaves</span><br><span class="line"> <span class="number">77</span>         params[<span class="string">'max_depth'</span>] = max_depth</span><br><span class="line"> <span class="number">78</span> </span><br><span class="line"> <span class="number">79</span>         cv_results = lgb.cv(</span><br><span class="line"> <span class="number">80</span>                             params,</span><br><span class="line"> <span class="number">81</span>                             lgb_train,</span><br><span class="line"> <span class="number">82</span>                             seed=<span class="number">2018</span>,</span><br><span class="line"> <span class="number">83</span>                             nfold=<span class="number">3</span>,</span><br><span class="line"> <span class="number">84</span>                             metrics=[<span class="string">'binary_error'</span>],</span><br><span class="line"> <span class="number">85</span>                             early_stopping_rounds=<span class="number">10</span>,</span><br><span class="line"> <span class="number">86</span>                             verbose_eval=<span class="keyword">True</span></span><br><span class="line"> <span class="number">87</span>                             )</span><br><span class="line"> <span class="number">88</span> </span><br><span class="line"> <span class="number">89</span>         mean_merror = pd.Series(cv_results[<span class="string">'binary_error-mean'</span>]).min()</span><br><span class="line"> <span class="number">90</span>         boost_rounds = pd.Series(cv_results[<span class="string">'binary_error-mean'</span>]).argmin()</span><br><span class="line"> <span class="number">91</span> </span><br><span class="line"> <span class="number">92</span>         <span class="keyword">if</span> mean_merror &lt; min_merror:</span><br><span class="line"> <span class="number">93</span>             min_merror = mean_merror</span><br><span class="line"> <span class="number">94</span>             best_params[<span class="string">'num_leaves'</span>] = num_leaves</span><br><span class="line"> <span class="number">95</span>             best_params[<span class="string">'max_depth'</span>] = max_depth</span><br><span class="line"> <span class="number">96</span> </span><br><span class="line"> <span class="number">97</span> params[<span class="string">'num_leaves'</span>] = best_params[<span class="string">'num_leaves'</span>]</span><br><span class="line"> <span class="number">98</span> params[<span class="string">'max_depth'</span>] = best_params[<span class="string">'max_depth'</span>]</span><br><span class="line"> <span class="number">99</span> </span><br><span class="line"><span class="number">100</span> <span class="comment"># 过拟合</span></span><br><span class="line"><span class="number">101</span> print(<span class="string">"调参2：降低过拟合"</span>)</span><br><span class="line"><span class="number">102</span> <span class="keyword">for</span> max_bin <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">255</span>,<span class="number">5</span>):</span><br><span class="line"><span class="number">103</span>     <span class="keyword">for</span> min_data_in_leaf <span class="keyword">in</span> range(<span class="number">10</span>,<span class="number">200</span>,<span class="number">5</span>):</span><br><span class="line"><span class="number">104</span>             params[<span class="string">'max_bin'</span>] = max_bin</span><br><span class="line"><span class="number">105</span>             params[<span class="string">'min_data_in_leaf'</span>] = min_data_in_leaf</span><br><span class="line"><span class="number">106</span> </span><br><span class="line"><span class="number">107</span>             cv_results = lgb.cv(</span><br><span class="line"><span class="number">108</span>                                 params,</span><br><span class="line"><span class="number">109</span>                                 lgb_train,</span><br><span class="line"><span class="number">110</span>                                 seed=<span class="number">42</span>,</span><br><span class="line"><span class="number">111</span>                                 nfold=<span class="number">3</span>,</span><br><span class="line"><span class="number">112</span>                                 metrics=[<span class="string">'binary_error'</span>],</span><br><span class="line"><span class="number">113</span>                                 early_stopping_rounds=<span class="number">3</span>,</span><br><span class="line"><span class="number">114</span>                                 verbose_eval=<span class="keyword">True</span></span><br><span class="line"><span class="number">115</span>                                 )</span><br><span class="line"><span class="number">116</span> </span><br><span class="line"><span class="number">117</span>             mean_merror = pd.Series(cv_results[<span class="string">'binary_error-mean'</span>]).min()</span><br><span class="line"><span class="number">118</span>             boost_rounds = pd.Series(cv_results[<span class="string">'binary_error-mean'</span>]).argmin()</span><br><span class="line"><span class="number">119</span> </span><br><span class="line"><span class="number">120</span>             <span class="keyword">if</span> mean_merror &lt; min_merror:</span><br><span class="line"><span class="number">121</span>                 min_merror = mean_merror</span><br><span class="line"><span class="number">122</span>                 best_params[<span class="string">'max_bin'</span>]= max_bin</span><br><span class="line"><span class="number">123</span>                 best_params[<span class="string">'min_data_in_leaf'</span>] = min_data_in_leaf</span><br><span class="line"><span class="number">124</span> </span><br><span class="line"><span class="number">125</span> params[<span class="string">'min_data_in_leaf'</span>] = best_params[<span class="string">'min_data_in_leaf'</span>]</span><br><span class="line"><span class="number">126</span> params[<span class="string">'max_bin'</span>] = best_params[<span class="string">'max_bin'</span>]</span><br><span class="line"><span class="number">127</span> </span><br><span class="line"><span class="number">128</span> print(<span class="string">"调参3：降低过拟合"</span>)</span><br><span class="line"><span class="number">129</span> <span class="keyword">for</span> feature_fraction <span class="keyword">in</span> [<span class="number">0.0</span>,<span class="number">0.1</span>,<span class="number">0.2</span>,<span class="number">0.3</span>,<span class="number">0.4</span>,<span class="number">0.5</span>,<span class="number">0.6</span>,<span class="number">0.7</span>,<span class="number">0.8</span>,<span class="number">0.9</span>,<span class="number">1.0</span>]:</span><br><span class="line"><span class="number">130</span>     <span class="keyword">for</span> bagging_fraction <span class="keyword">in</span> [<span class="number">0.0</span>,<span class="number">0.1</span>,<span class="number">0.2</span>,<span class="number">0.3</span>,<span class="number">0.4</span>,<span class="number">0.5</span>,<span class="number">0.6</span>,<span class="number">0.7</span>,<span class="number">0.8</span>,<span class="number">0.9</span>,<span class="number">1.0</span>]:</span><br><span class="line"><span class="number">131</span>         <span class="keyword">for</span> bagging_freq <span class="keyword">in</span> range(<span class="number">0</span>,<span class="number">50</span>,<span class="number">5</span>):</span><br><span class="line"><span class="number">132</span>             params[<span class="string">'feature_fraction'</span>] = feature_fraction</span><br><span class="line"><span class="number">133</span>             params[<span class="string">'bagging_fraction'</span>] = bagging_fraction</span><br><span class="line"><span class="number">134</span>             params[<span class="string">'bagging_freq'</span>] = bagging_freq</span><br><span class="line"><span class="number">135</span> </span><br><span class="line"><span class="number">136</span>             cv_results = lgb.cv(</span><br><span class="line"><span class="number">137</span>                                 params,</span><br><span class="line"><span class="number">138</span>                                 lgb_train,</span><br><span class="line"><span class="number">139</span>                                 seed=<span class="number">42</span>,</span><br><span class="line"><span class="number">140</span>                                 nfold=<span class="number">3</span>,</span><br><span class="line"><span class="number">141</span>                                 metrics=[<span class="string">'binary_error'</span>],</span><br><span class="line"><span class="number">142</span>                                 early_stopping_rounds=<span class="number">3</span>,</span><br><span class="line"><span class="number">143</span>                                 verbose_eval=<span class="keyword">True</span></span><br><span class="line"><span class="number">144</span>                                 )</span><br><span class="line"><span class="number">145</span> </span><br><span class="line"><span class="number">146</span>             mean_merror = pd.Series(cv_results[<span class="string">'binary_error-mean'</span>]).min()</span><br><span class="line"><span class="number">147</span>             boost_rounds = pd.Series(cv_results[<span class="string">'binary_error-mean'</span>]).argmin()</span><br><span class="line"><span class="number">148</span> </span><br><span class="line"><span class="number">149</span>             <span class="keyword">if</span> mean_merror &lt; min_merror:</span><br><span class="line"><span class="number">150</span>                 min_merror = mean_merror</span><br><span class="line"><span class="number">151</span>                 best_params[<span class="string">'feature_fraction'</span>] = feature_fraction</span><br><span class="line"><span class="number">152</span>                 best_params[<span class="string">'bagging_fraction'</span>] = bagging_fraction</span><br><span class="line"><span class="number">153</span>                 best_params[<span class="string">'bagging_freq'</span>] = bagging_freq</span><br><span class="line"><span class="number">154</span> </span><br><span class="line"><span class="number">155</span> params[<span class="string">'feature_fraction'</span>] = best_params[<span class="string">'feature_fraction'</span>]</span><br><span class="line"><span class="number">156</span> params[<span class="string">'bagging_fraction'</span>] = best_params[<span class="string">'bagging_fraction'</span>]</span><br><span class="line"><span class="number">157</span> params[<span class="string">'bagging_freq'</span>] = best_params[<span class="string">'bagging_freq'</span>]</span><br><span class="line"><span class="number">158</span> </span><br><span class="line"><span class="number">159</span> print(<span class="string">"调参4：降低过拟合"</span>)</span><br><span class="line"><span class="number">160</span> <span class="keyword">for</span> lambda_l1 <span class="keyword">in</span> [<span class="number">0.0</span>,<span class="number">0.1</span>,<span class="number">0.2</span>,<span class="number">0.3</span>,<span class="number">0.4</span>,<span class="number">0.5</span>,<span class="number">0.6</span>,<span class="number">0.7</span>,<span class="number">0.8</span>,<span class="number">0.9</span>,<span class="number">1.0</span>]:</span><br><span class="line"><span class="number">161</span>     <span class="keyword">for</span> lambda_l2 <span class="keyword">in</span> [<span class="number">0.0</span>,<span class="number">0.1</span>,<span class="number">0.2</span>,<span class="number">0.3</span>,<span class="number">0.4</span>,<span class="number">0.5</span>,<span class="number">0.6</span>,<span class="number">0.7</span>,<span class="number">0.8</span>,<span class="number">0.9</span>,<span class="number">1.0</span>]:</span><br><span class="line"><span class="number">162</span>         <span class="keyword">for</span> min_split_gain <span class="keyword">in</span> [<span class="number">0.0</span>,<span class="number">0.1</span>,<span class="number">0.2</span>,<span class="number">0.3</span>,<span class="number">0.4</span>,<span class="number">0.5</span>,<span class="number">0.6</span>,<span class="number">0.7</span>,<span class="number">0.8</span>,<span class="number">0.9</span>,<span class="number">1.0</span>]:</span><br><span class="line"><span class="number">163</span>             params[<span class="string">'lambda_l1'</span>] = lambda_l1</span><br><span class="line"><span class="number">164</span>             params[<span class="string">'lambda_l2'</span>] = lambda_l2</span><br><span class="line"><span class="number">165</span>             params[<span class="string">'min_split_gain'</span>] = min_split_gain</span><br><span class="line"><span class="number">166</span> </span><br><span class="line"><span class="number">167</span>             cv_results = lgb.cv(</span><br><span class="line"><span class="number">168</span>                                 params,</span><br><span class="line"><span class="number">169</span>                                 lgb_train,</span><br><span class="line"><span class="number">170</span>                                 seed=<span class="number">42</span>,</span><br><span class="line"><span class="number">171</span>                                 nfold=<span class="number">3</span>,</span><br><span class="line"><span class="number">172</span>                                 metrics=[<span class="string">'binary_error'</span>],</span><br><span class="line"><span class="number">173</span>                                 early_stopping_rounds=<span class="number">3</span>,</span><br><span class="line"><span class="number">174</span>                                 verbose_eval=<span class="keyword">True</span></span><br><span class="line"><span class="number">175</span>                                 )</span><br><span class="line"><span class="number">176</span> </span><br><span class="line"><span class="number">177</span>             mean_merror = pd.Series(cv_results[<span class="string">'binary_error-mean'</span>]).min()</span><br><span class="line"><span class="number">178</span>             boost_rounds = pd.Series(cv_results[<span class="string">'binary_error-mean'</span>]).argmin()</span><br><span class="line"><span class="number">179</span> </span><br><span class="line"><span class="number">180</span>             <span class="keyword">if</span> mean_merror &lt; min_merror:</span><br><span class="line"><span class="number">181</span>                 min_merror = mean_merror</span><br><span class="line"><span class="number">182</span>                 best_params[<span class="string">'lambda_l1'</span>] = lambda_l1</span><br><span class="line"><span class="number">183</span>                 best_params[<span class="string">'lambda_l2'</span>] = lambda_l2</span><br><span class="line"><span class="number">184</span>                 best_params[<span class="string">'min_split_gain'</span>] = min_split_gain</span><br><span class="line"><span class="number">185</span> </span><br><span class="line"><span class="number">186</span> params[<span class="string">'lambda_l1'</span>] = best_params[<span class="string">'lambda_l1'</span>]</span><br><span class="line"><span class="number">187</span> params[<span class="string">'lambda_l2'</span>] = best_params[<span class="string">'lambda_l2'</span>]</span><br><span class="line"><span class="number">188</span> params[<span class="string">'min_split_gain'</span>] = best_params[<span class="string">'min_split_gain'</span>]</span><br><span class="line"><span class="number">189</span> </span><br><span class="line"><span class="number">190</span> </span><br><span class="line"><span class="number">191</span> print(best_params)</span><br><span class="line"><span class="number">192</span> </span><br><span class="line"><span class="number">193</span> <span class="comment">### 训练</span></span><br><span class="line"><span class="number">194</span> params[<span class="string">'learning_rate'</span>]=<span class="number">0.01</span></span><br><span class="line"><span class="number">195</span> lgb.train(</span><br><span class="line"><span class="number">196</span>           params,                     <span class="comment"># 参数字典</span></span><br><span class="line"><span class="number">197</span>           lgb_train,                  <span class="comment"># 训练集</span></span><br><span class="line"><span class="number">198</span>           valid_sets=lgb_eval,        <span class="comment"># 验证集</span></span><br><span class="line"><span class="number">199</span>           num_boost_round=<span class="number">2000</span>,       <span class="comment"># 迭代次数</span></span><br><span class="line"><span class="number">200</span>           early_stopping_rounds=<span class="number">50</span>    <span class="comment"># 早停次数</span></span><br><span class="line"><span class="number">201</span>           )</span><br><span class="line"><span class="number">202</span> </span><br><span class="line"><span class="number">203</span> <span class="comment">### 线下预测</span></span><br><span class="line"><span class="number">204</span> <span class="keyword">print</span> (<span class="string">"线下预测"</span>)</span><br><span class="line"><span class="number">205</span> preds_offline = lgb.predict(offline_test_X, num_iteration=lgb.best_iteration) <span class="comment"># 输出概率</span></span><br><span class="line"><span class="number">206</span> offline=offline_test[[<span class="string">'instance_id'</span>,<span class="string">'is_trade'</span>]]</span><br><span class="line"><span class="number">207</span> offline[<span class="string">'preds'</span>]=preds_offline</span><br><span class="line"><span class="number">208</span> offline.is_trade = offline[<span class="string">'is_trade'</span>].astype(np.float64)</span><br><span class="line"><span class="number">209</span> print(<span class="string">'log_loss'</span>, metrics.log_loss(offline.is_trade, offline.preds))</span><br><span class="line"><span class="number">210</span> </span><br><span class="line"><span class="number">211</span> <span class="comment">### 线上预测</span></span><br><span class="line"><span class="number">212</span> print(<span class="string">"线上预测"</span>)</span><br><span class="line"><span class="number">213</span> preds_online =  lgb.predict(online_test_X, num_iteration=lgb.best_iteration)  <span class="comment"># 输出概率</span></span><br><span class="line"><span class="number">214</span> online=online_test[[<span class="string">'instance_id'</span>]]</span><br><span class="line"><span class="number">215</span> online[<span class="string">'preds'</span>]=preds_online</span><br><span class="line"><span class="number">216</span> online.rename(columns=&#123;<span class="string">'preds'</span>:<span class="string">'predicted_score'</span>&#125;,inplace=<span class="keyword">True</span>)           <span class="comment"># 更改列名</span></span><br><span class="line"><span class="number">217</span> online.to_csv(<span class="string">"./data/20180405.txt"</span>,index=<span class="keyword">None</span>,sep=<span class="string">' '</span>)                   <span class="comment"># 保存结果</span></span><br><span class="line"><span class="number">218</span> </span><br><span class="line"><span class="number">219</span> <span class="comment">### 保存模型</span></span><br><span class="line"><span class="number">220</span> <span class="keyword">from</span> sklearn.externals <span class="keyword">import</span> joblib</span><br><span class="line"><span class="number">221</span> joblib.dump(lgb,<span class="string">'lgb.pkl'</span>)</span><br><span class="line"><span class="number">222</span> </span><br><span class="line"><span class="number">223</span> <span class="comment">### 特征选择</span></span><br><span class="line"><span class="number">224</span> df = pd.DataFrame(X_train.columns.tolist(), columns=[<span class="string">'feature'</span>])</span><br><span class="line"><span class="number">225</span> df[<span class="string">'importance'</span>]=list(lgb.feature_importance())                           <span class="comment"># 特征分数</span></span><br><span class="line"><span class="number">226</span> df = df.sort_values(by=<span class="string">'importance'</span>,ascending=<span class="keyword">False</span>)                      <span class="comment"># 特征排序</span></span><br><span class="line"><span class="number">227</span> df.to_csv(<span class="string">"./data/feature_score_20180331.csv"</span>,index=<span class="keyword">None</span>,encoding=<span class="string">'gbk'</span>)  <span class="comment"># 保存分数</span></span><br></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      <a data-url="//harold.me/2018/06/07/lightGbm简要原理/" data-id="cji4l7i6r000vcf96j63n8iie" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/机器学习/">机器学习</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2018/06/07/数据算法——Spark的二次排序解决方案/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          数据算法——Spark的二次排序解决方案
        
      </div>
    </a>
  
  
    <a href="/2018/06/04/数据算法-二次排序/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">数据算法——Hadoop的二次排序解决方案</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Crunch/">Crunch</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ElasticSearch/">ElasticSearch</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/HBase/">HBase</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/HTTP/">HTTP</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hadoop/">Hadoop</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hive/">Hive</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/JVM/">JVM</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Java/">Java</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/MLib/">MLib</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/MLlib/">MLlib</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Mahout/">Mahout</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/MySQL/">MySQL</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Spark/">Spark</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/scala/">scala</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/分布式算法/">分布式算法</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/区块链/">区块链</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/协同过滤/">协同过滤</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/多线程/">多线程</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/大数据/">大数据</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/字符串匹配/">字符串匹配</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/并发/">并发</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/推荐系统/">推荐系统</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/数据库/">数据库</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/机器学习/">机器学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/算法/">算法</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/随笔/">随笔</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Crunch/" style="font-size: 10px;">Crunch</a> <a href="/tags/ElasticSearch/" style="font-size: 10px;">ElasticSearch</a> <a href="/tags/HBase/" style="font-size: 10px;">HBase</a> <a href="/tags/HTTP/" style="font-size: 10px;">HTTP</a> <a href="/tags/Hadoop/" style="font-size: 17.5px;">Hadoop</a> <a href="/tags/Hive/" style="font-size: 11.25px;">Hive</a> <a href="/tags/JVM/" style="font-size: 10px;">JVM</a> <a href="/tags/Java/" style="font-size: 18.75px;">Java</a> <a href="/tags/MLib/" style="font-size: 10px;">MLib</a> <a href="/tags/MLlib/" style="font-size: 10px;">MLlib</a> <a href="/tags/Mahout/" style="font-size: 10px;">Mahout</a> <a href="/tags/MySQL/" style="font-size: 12.5px;">MySQL</a> <a href="/tags/Spark/" style="font-size: 10px;">Spark</a> <a href="/tags/scala/" style="font-size: 15px;">scala</a> <a href="/tags/分布式算法/" style="font-size: 11.25px;">分布式算法</a> <a href="/tags/区块链/" style="font-size: 11.25px;">区块链</a> <a href="/tags/协同过滤/" style="font-size: 11.25px;">协同过滤</a> <a href="/tags/多线程/" style="font-size: 16.25px;">多线程</a> <a href="/tags/大数据/" style="font-size: 20px;">大数据</a> <a href="/tags/字符串匹配/" style="font-size: 10px;">字符串匹配</a> <a href="/tags/并发/" style="font-size: 16.25px;">并发</a> <a href="/tags/推荐系统/" style="font-size: 11.25px;">推荐系统</a> <a href="/tags/数据库/" style="font-size: 13.75px;">数据库</a> <a href="/tags/机器学习/" style="font-size: 17.5px;">机器学习</a> <a href="/tags/算法/" style="font-size: 13.75px;">算法</a> <a href="/tags/随笔/" style="font-size: 11.25px;">随笔</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/06/">June 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/05/">May 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">April 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/03/">March 2018</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2018/06/07/数据算法——Spark的二次排序解决方案/">数据算法——Spark的二次排序解决方案</a>
          </li>
        
          <li>
            <a href="/2018/06/07/lightGbm简要原理/">lightGbm简要原理</a>
          </li>
        
          <li>
            <a href="/2018/06/04/数据算法-二次排序/">数据算法——Hadoop的二次排序解决方案</a>
          </li>
        
          <li>
            <a href="/2018/06/04/ES分词器/">ES分词器</a>
          </li>
        
          <li>
            <a href="/2018/06/04/hexo系列问题之我们换了电脑怎么办/">hexo系列问题之我们换了电脑怎么办</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2018 李贺<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>