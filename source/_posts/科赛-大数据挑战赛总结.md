---
title: 科赛----大数据挑战赛总结
date: 2018-07-03 19:53:58
tags: [机器学习, 大数据]
---

这个比赛是我除了kaggle两个练手比赛之外第一次参加的比赛,历时一个月左右,有遗憾也有收获,不过收获总是大于遗憾的,作为第一个从头到尾参与的比赛,从数据集的划分,特征的生成到模型的选取与调优,一点一滴都是收获,最终成绩(初赛B榜)0.81670099,虽然不是特别高,但是思路总是值得记录的.

#### 一. 赛题

**赛题描述**:

      本次大赛基于脱敏和采样后的数据信息，预测未来一段时间活跃的用户。参赛队伍需要设计相应的算法进行数据分析和处理，比赛结果按照指定的评价指标使用在线评测数据进行评测和排名，得分最优者获胜。

<!-- more--> 

**数据说明**

​      大赛提供脱敏和采样后用户行为数据，日期信息进行统一编号，第一天编号为 01， 第二天为 02， 以此类推，所有文件中列使用 tab 分割。

1.注册日志（user_register_log.txt)

| 列名          | 类型   | 说明                   | 示例          |
| ------------- | ------ | ---------------------- | ------------- |
| user_id       | Int    | 用户唯一标识（脱敏后） | 666           |
| register_day  | String | 日期                   | 01, 02 ..  30 |
| register_type | Int    | 来源渠道（脱敏后）     | 0             |
| device type   | Int    | 设备类型（脱敏后）     | 0             |

2.APP启动日志（app_launch_log.txt)

| 列名    | 类型   | 说明                   | 示例        |
| ------- | ------ | ---------------------- | ----------- |
| user_id | Int    | 用户唯一标识（脱敏后） | 666         |
| day     | String | 日期                   | 01, 02 ..30 |

3.拍摄日志（video_create_log.txt）

| 列名    | 类型   | 说明                   | 示例         |
| ------- | ------ | ---------------------- | ------------ |
| user_id | Int    | 用户唯一标识（脱敏后） | 666          |
| day     | String | 拍摄日期               | 01, 02 .. 30 |

4.行为日志（user_activity_log.txt

| 列名        | 类型   | 说明                                                         | 示例   |
| ----------- | ------ | ------------------------------------------------------------ | ------ |
| user_id     | Int    | 用户唯一标识（脱敏后）                                       | 666    |
| day         | String | 日期                                                         | 01, 02 |
| page        | Int    | 行为发生的页面。每个数字分别对应“关注页”、”个人主页“、”发现页“、”同城页“或”其他页“中的一个 | 1      |
| video_id    | Int    | video id（脱敏后）                                           | 333    |
| author_id   | Int    | 作者 id（脱敏后）                                            | 999    |
| action_type | Int    | 用户行为类型。每个数字分别对应“播放“、”关注“、”点赞“、”转发“、”举报“和”减少此类作品“中的一个 | 1      |

**评估标准**

设参赛选手提交的用户集合为 M，实际上未来 7 天内使用过快手的用户集合为 N ，且集合 N 是提供给选手的注册用户的子集。最终使用 F1 Score 作为参赛选手得分。F1 Score 越大，代表结果越优，排名越靠前。

#### 二.思路

​	首先看这个题目是一个预测问题,根据前30天的表现预测用户之后七天是否活跃.在我看到baseline之前我是这样想的:*先提一些特征,然后利用最简单的 K-Means (K = 2) 做一下聚类 ,然后选出一类作为活跃的类,再从活跃的类中选出登录时间比较靠靠近最后一天的用户作为预测出来的活跃用户*.在我正在验证这个想法的半途中自己的Linux系统挂了,重装之后之前的工作就都没了,一度想弃赛...

**1.划分数据集**

​	好在加了比赛的群,而且科赛有讨论组,我发现大家和我想的完全不一样(果然新手和老鸟差距太大).这是一个分类问题,但是主办方并没有给出前三十天用户是否活跃的标签,因此需要我们自己去构造标签.因为数据是和时间完全相关的,因此使用滑窗法来构造标签.比如,使用第17-23的数据来标注第1-16天的数据,使用这种方法可以构造很多训练集.我采用了大多数人使用的构造方法:

train_set1: 用1-16天构造用户集, 1-16构造特征, 17-23构造标签

train_set2: 用1-23天构造用户集, 8-23构造特征, 24-30 构造标签

test_set: 1-30天构造用户集,15-30构造特征, 预测下月1-7

​	在划分数据集之前,先构造了两个全局特征,根据user_activity_log表观看视频的视频id和作者id(video_id和author_id)可以得到某个video_id和user_id的火热程度,同时根据video_create_log的user_id和user_activity_log的author_id可以构造用户发作为一个发布者的火热程度.

具体代码如下:

```python
import numpy as np
import pandas as pd
from pandas import DataFrame

path ="data/"
app = pd.read_table(path+'app_launch_log.txt',names=['user_id','day'],encoding='utf-8',sep='\t',)
user_act = pd.read_table(path+'user_activity_log.txt',names=['user_id','day','page','video_id','author_id','action_type'],encoding='utf-8',sep='\t')
user_reg = pd.read_table(path+'user_register_log.txt',names=['user_id','register_day','register_type','device_type'],encoding='utf-8',sep='\t')
video = pd.read_table(path+'video_create_log.txt',names=['user_id','day'],encoding='utf-8',sep='\t')
#讲txt文件转为csv
app.to_csv('data/app_lauch_log.csv',index=False)
user_act.to_csv('data/user_activity_log.csv',index=False)
user_reg.to_csv('data/user_register_log.csv',index=False)
video.to_csv('data/video_create_log.csv',index=False)

#构造特征:vid_hotness和video_norm_hot
vid = pd.DataFrame(user_act.video_id)
vid['hot'] = 1
v = vid.groupby('video_id').count()
v.reset_index(inplace=True)
v_norm = (v.hot - v.hot.min()) / (v.hot.max() - v.hot.min())
v['video_norm_hot']= v_norm
v.rename(columns={'hot':'vid_hotness'}, inplace=True)
user_act = pd.merge(user_act,v,on='video_id',how='left')

#构造特征:auth_hotness和author_norm_hot
aut = pd.DataFrame(user_act.author_id)
aut['hot'] = 1
a = aut.groupby('author_id').count()
a.reset_index(inplace=True)
a_norm = (a.hot - a.hot.min()) / (a.hot.max() - a.hot.min())
a['author_norm_hot'] = a_norm
a.rename(columns={'hot':'auth_hotness'}, inplace=True)
user_act = pd.merge(user_act,a,on='author_id',how='left')
user_act.to_csv('data/user_activity_log.csv', index=False)
user_act.head()

#构造特征:hotness_as_creater和norm_hotness_as_creater
a.rename(columns={'auth_hotness':'hotness_as_creater','author_id':'user_id','author_norm_hot':'norm_hotness_as_creater'}, inplace=True)
video = pd.merge(video, a, on='user_id',how='left')
video.head()
video.to_csv('data/video_create_log.csv',index=False)

A_train = 'data/1_data_train_'
B_train = 'data/2_data_train_'
C_train = 'data/3_data_train_'
A_test = 'data/1_data_test_'
B_test = 'data/2_data_test_'

#划分数据
def cut_data_as_time(new_data_path, begin_day, end_day):
    temp_register = user_reg[(user_reg['register_day'] >= 1) & (user_reg['register_day'] <= end_day)]
    temp_create = video[(video['day'] >= begin_day) & (video['day'] <= end_day)]
    temp_launch = app[(app['day'] >= begin_day) & (app['day'] <= end_day)]
    temp_activity = user_act[(user_act['day'] >= begin_day) & (user_act['day'] <= end_day)]
    
    temp_register.to_csv(new_data_path + "register.csv", index = False)
    temp_activity.to_csv(new_data_path + "activity.csv", index = False)
    temp_launch.to_csv(new_data_path + "launch.csv", index = False)
    temp_create.to_csv(new_data_path + "create.csv", index = False)
    print('根据起始和结束时间将数据集取出完成,存入:',new_data_path)

def generate_dataset():
    print('开始划分数据集....')
    begin_day = 1
    end_day = 16
    cut_data_as_time(A_train,begin_day, end_day)
    begin_day = 17
    end_day = 23
    cut_data_as_time(A_test,begin_day, end_day)
    print('第一数据集划分完成')
    begin_day = 8
    end_day = 23
    cut_data_as_time(B_train,begin_day, end_day)
    begin_day = 24
    end_day = 30
    cut_data_as_time(B_test,begin_day, end_day)
    print('第二数据集划分完成')
    begin_day = 15
    end_day = 30
    cut_data_as_time(C_train,begin_day, end_day)
    print('第三数据集划分完成')
    print('全部数据集划分完成.....')
```

**2.构造数据集与特征**

​	在这一步我将要利用第一步划分好的窗口来给测试集打标签并构造特征.

​	打标签的功能比较好实现,只要前一个\指定数据集中的用户(如1-16中的用户)出现在打标签数据集中(如17-23),就标记该用户活跃状态为1,否则为0.

​	本次比赛我一共构造了160多个特征,他们来自数据的均值,方差,最大最小值,最近几天的活动次数,某一种活动的次数以及相邻几次活动的差值特性等.说一说遇到的坑,在最初几次提特征时提了30总的launch次数,activity次数等全局特征,在线下f1值猛增,线上提交却发现成绩陡降,仔细一想,自己利用滑窗法构造数据集进行训练,这种全局的数据一定会导致leak,因此万万要不得,算是吃一堑长一智吧.看到初赛后有人提了300多个特征,包括几天之内连续操作的次数,带windows权重的特征,带register_time权重的特征等等,只能说在下佩服,一定好好拜读代码.

​	一般来说，数据挖掘比赛等于特征构造比赛。特征构造的好坏决定了模型的上限，也基本能够保证模型的下限。但是特征也不是越多越好,就像我一样,虽然构造了那么多特征,但是最后发现全特征还不如最重要的30多个特征的效果(没错,最终只用了三十多个有效特征).

​	代码如下:

```python
import numpy as np
import pandas as pd
from pandas import DataFrame
from scipy.stats import stats
from collections import Counter

one_dataSet_train_path = "data/1_data_train_"
one_dataSet_test_path = 'data/1_data_test_'
two_dataSet_train_path = 'data/2_data_train_'
two_dataSet_test_path = 'data/2_data_test_'
three_dataSet_train_path = 'data/3_data_train_'
train_path = 'data/train_and_test/train.csv'
test_path = 'data/train_and_test/test.csv'

register = 'register.csv'
create = 'create.csv'
launch = 'launch.csv'
activity = 'activity.csv'

#构建训练集与测试集与特征
#获取所有id,查看对应id是否在测试集中出现过
def get_train_label(train_path, test_path):
    train_reg = pd.read_csv(train_path + register, usecols=['user_id'])
    train_data_id = np.unique(train_reg)
    
    test_cre = pd.read_csv(test_path + create, usecols=['user_id'])
    test_lau = pd.read_csv(test_path + launch, usecols=['user_id'])
    test_act = pd.read_csv(test_path + activity, usecols=['user_id'])
    test_data_id = np.unique(pd.concat([test_act, test_cre, test_lau]))
    
    train_label = []
    for i in train_data_id:
        if i in test_data_id:
            train_label.append(1)
        else:
            train_label.append(0)
    train_data = pd.DataFrame()
    train_data['user_id'] = train_data_id
    train_data['label'] = train_label
    return train_data

def get_test(test_path):
    test_reg = pd.read_csv(test_path + register, usecols=['user_id'])
    test_cre = pd.read_csv(test_path + create, usecols=['user_id'])
    test_lau = pd.read_csv(test_path + launch, usecols=['user_id'])
    test_act = pd.read_csv(test_path + activity, usecols=['user_id'])
    test_data_id = np.unique(test_reg)
    test_data = pd.DataFrame()
    test_data['user_id'] = test_data_id
    return test_data

def get_create_feature(row):
    feature = pd.Series()
    feature['user_id'] = list(row['user_id'])[0]
    feature['create_count_recent_days'] = int (pd.DataFrame(row['user_id']).count())
    diff = list(row['max_day'])[0] - list(row['register_day'])[0] + 1
    if diff >= 15:
        diff = 15
    feature['create_count_in_last_3_day'] = int (row.day[row.day >= (list(row['max_day'])[0] - 2)].count())
    feature['create_count_in_last_5_day'] = int (row.day[row.day >= (list(row['max_day'])[0] - 4)].count())
    feature['create_count_in_last_7_day'] = int (row.day[row.day >= (list(row['max_day'])[0] - 6)].count())
    feature['create_count_in_last_9_day'] = int (row.day[row.day >= (list(row['max_day'])[0] - 8)].count())
    
    feature['create_day_mean'] = float (feature['create_count_recent_days']/(diff+1))
    feature['create_day_max'] = int(pd.DataFrame(row['day']).max())
    feature['create_day_min'] = int (pd.DataFrame(row['day']).min())
    feature['create_day_max_sub_min'] = feature['create_day_max'] - feature['create_day_min']
    feature['create_day_std'] = float (pd.DataFrame(row['day']).std())
    feature['create_day_var'] = float (pd.DataFrame(row['day']).var())
    feature['create_day_skew'] = float (pd.DataFrame(row['day']).skew())
    feature['create_day_kur'] = float (pd.DataFrame(row['day']).kurt())
    feature['last_day_cut_max_day'] = list(row['max_day'])[0] - feature['create_day_max']
    feature['first_day_cut_register_day'] = feature['create_day_min'] - list(row['register_day'])[0]
    feature['max_create_times_one_day'] = int (row.groupby('day').count()['user_id'].max())
    
    if row.day.count() > 1:
        a = np.array(row.day)[:-1]
        b = np.array(row.day)[1:]
        tmp = pd.DataFrame(b-a)
        feature['create_day_diff_mean'] = float (tmp.mean())
        feature['create_day_diff_max'] = int (tmp.max())
        feature['create_day_diff_min'] = int (tmp.min())
        feature['create_day_diff_std'] = float (tmp.std())
        feature['create_day_diff_var'] = float (tmp.var())
    else:
        feature['create_day_diff_mean'] = np.nan
        feature['create_day_diff_max'] = np.nan
        feature['create_day_diff_min'] = np.nan
        feature['create_day_diff_std'] = np.nan
        feature['create_day_diff_var'] = np.nan
    return feature      

def get_launch_feature(row):
    feature = pd.Series()
    feature['user_id'] = list(row['user_id'])[0]
    feature['launch_count_recent_days'] = int (pd.DataFrame(row['user_id']).count())
    diff = list(row['max_day'])[0] - list(row['register_day'])[0] + 1
    feature['reg_now_diff'] = diff
    if diff >= 15:
        diff = 15
    feature['launch_count_in_last_3_day'] = int (row.day[row.day >= (list(row['max_day'])[0] - 2)].count())
    feature['launch_count_in_last_5_day'] = int (row.day[row.day >= (list(row['max_day'])[0] - 4)].count())
    feature['launch_count_in_last_7_day'] = int (row.day[row.day >= (list(row['max_day'])[0] - 6)].count())
    feature['launch_count_in_last_9_day'] = int (row.day[row.day >= (list(row['max_day'])[0] - 8)].count())    
    feature['launch_day_mean'] = float (feature['launch_count_recent_days']/(diff+1))
    feature['launch_day_max'] = int (pd.DataFrame(row['day']).max())
    feature['launch_day_min'] = int (pd.DataFrame(row['day']).min())
    feature['launch_day_max_sub_min'] = feature['launch_day_max'] - feature['launch_day_min']
    feature['launch_day_std'] = float (pd.DataFrame(row['day']).std())
    feature['launch_day_var'] = float (pd.DataFrame(row['day']).var())
    feature['launch_day_kur'] = float (pd.DataFrame(row['day']).kurt())
    feature['launch_day_skew'] = float (pd.DataFrame(row['day']).skew())
    feature['last_launch_day_cut_max_day'] = list(row['max_day'])[0] - feature['launch_day_max']
    feature['first_launch_day_cut_register_day'] = feature['launch_day_min'] - list(row['register_day'])[0]
    feature['launch_mean_cut_max_day'] = feature['launch_day_mean'] - list(row['max_day'])[0]
    feature['max_launch_times_one_day'] = int (row.groupby('day').count()['user_id'].max())
    if row.day.count() > 1:
        a = np.array(row.day)[:-1]
        b = np.array(row.day)[1:]
        tmp = pd.DataFrame(b-a)
        feature['launch_day_diff_mean'] = float (tmp.mean())
        feature['launch_day_diff_max']  = int (tmp.max())
        feature['launch_day_diff_min']  = int (tmp.min())
        feature['launch_day_diff_std'] = float (tmp.std())
        feature['launch_day_diff_var'] = float (tmp.var())
    else:
        feature['launch_day_diff_mean'] = np.nan
        feature['launch_day_diff_max']  = np.nan
        feature['launch_day_diff_min']  = np.nan
        feature['launch_day_diff_std'] = np.nan
        feature['launch_day_diff_var'] = np.nan
    return feature   

def get_activity_feature(row):
    feature = pd.Series()
    feature['user_id'] = list(row['user_id'])[0]
    diff = list(row['max_day'])[0] - list(row['register_day'])[0] + 1
    if diff >= 15:
        diff = 15
    feature['act_count_in_last_3_day'] = int (row.day[row.day >= (list(row['max_day'])[0] - 2)].count())
    feature['act_count_in_last_5_day'] = int (row.day[row.day >= (list(row['max_day'])[0] - 4)].count())
    feature['act_count_in_last_7_day'] = int (row.day[row.day >= (list(row['max_day'])[0] - 6)].count())
    feature['act_count_in_last_9_day'] = int (row.day[row.day >= (list(row['max_day'])[0] - 8)].count())
    feature['activity_count'] = int (pd.DataFrame(row['user_id']).count())
    feature['activity_day_mean'] = float (feature['activity_count']/(diff+1))
    feature['activity_day_max'] = int (row['day'].max())
    feature['activity_day_min'] = int (row['day'].min())
    feature['activity_day_std'] = float (row['day'].std())
    feature['activity_day_var'] = float (row['day'].var())
    feature['activity_day_ske'] = float (row['day'].skew())
    feature['activity_day_kur'] = float (row['day'].skew())
    feature['activity_day_max_cut_min'] =  feature['activity_day_max'] - feature['activity_day_min']
    feature['activity_day_cut_max_day'] = list(row['max_day'])[0] - feature['activity_day_max']
    feature['activity_day_cut_register_day'] = feature['activity_day_min'] - list(row['register_day'])[0]
    feature['act_sub_register'] = feature['activity_day_min'] - list(row['max_day'])[0]
    feature['act_count_in_last_3_day'] = int (row.day[row.day >= (list(row['max_day'])[0] - 2)].count())
    feature['0_page_count'] = int (row.page[row.page==0].count())
    feature['1_page_count'] = int (row.page[row.page==1].count())
    feature['2_page_count'] = int (row.page[row.page==2].count())
    feature['3_page_count'] = int (row.page[row.page==3].count())
    feature['4_page_count'] = int (row.page[row.page==4].count())
    
    feature['0_page_count_in_3_days'] = int (row.page[(row.page==0)&(row.day >= (list(row['max_day'])[0] - 2))].count())
    feature['1_page_count_in_3_days'] = int (row.page[(row.page==1)&(row.day >= (list(row['max_day'])[0] - 2))].count())
    feature['2_page_count_in_3_days'] = int (row.page[(row.page==2)&(row.day >= (list(row['max_day'])[0] - 2))].count())
    feature['3_page_count_in_3_days'] = int (row.page[(row.page==3)&(row.day >= (list(row['max_day'])[0] - 2))].count())
    feature['4_page_count_in_3_days'] = int (row.page[(row.page==4)&(row.day >= (list(row['max_day'])[0] - 2))].count())
    
    feature['0_page_count_in_5_days'] = int (row.page[(row.page==0)&(row.day >= (list(row['max_day'])[0] - 4))].count())
    feature['1_page_count_in_5_days'] = int (row.page[(row.page==1)&(row.day >= (list(row['max_day'])[0] - 4))].count())
    feature['2_page_count_in_5_days'] = int (row.page[(row.page==2)&(row.day >= (list(row['max_day'])[0] - 4))].count())
    feature['3_page_count_in_5_days'] = int (row.page[(row.page==3)&(row.day >= (list(row['max_day'])[0] - 4))].count())
    feature['4_page_count_in_5_days'] = int (row.page[(row.page==4)&(row.day >= (list(row['max_day'])[0] - 4))].count())
    
    feature['0_page_count_in_7_days'] = int (row.page[(row.page==0)&(row.day >= (list(row['max_day'])[0] - 6))].count())
    feature['1_page_count_in_7_days'] = int (row.page[(row.page==1)&(row.day >= (list(row['max_day'])[0] - 6))].count())
    feature['2_page_count_in_7_days'] = int (row.page[(row.page==2)&(row.day >= (list(row['max_day'])[0] - 6))].count())
    feature['3_page_count_in_7_days'] = int (row.page[(row.page==3)&(row.day >= (list(row['max_day'])[0] - 6))].count())
    feature['4_page_count_in_7_days'] = int (row.page[(row.page==4)&(row.day >= (list(row['max_day'])[0] - 6))].count())
    
    feature['0_page_count_div_sum'] = float((feature['0_page_count'] + 1)/(feature['activity_count'] + 1))
    feature['1_page_count_div_sum'] = float((feature['1_page_count'] + 1)/(feature['activity_count'] + 1))
    feature['2_page_count_div_sum'] = float((feature['2_page_count'] + 1)/(feature['activity_count'] + 1))
    feature['3_page_count_div_sum'] = float((feature['3_page_count'] + 1)/(feature['activity_count'] + 1))
    feature['4_page_count_div_sum'] = float((feature['4_page_count'] + 1)/(feature['activity_count'] + 1))
    
    feature['0_action_count'] = int (row.action_type[row.action_type==0].count())
    feature['1_action_count'] = int (row.action_type[row.action_type==1].count())
    feature['2_action_count'] = int (row.action_type[row.action_type==2].count())
    feature['3_action_count'] = int (row.action_type[row.action_type==3].count())
    feature['4_action_count'] = int (row.action_type[row.action_type==4].count())
    feature['5_action_count'] = int (row.action_type[row.action_type==5].count())
    
    feature['0_action_count_in_3_days'] = int (row.action_type[(row.action_type==0)&(row.day >= (list(row['max_day'])[0] - 2))].count())
    feature['1_action_count_in_3_days'] = int (row.action_type[(row.action_type==1)&(row.day >= (list(row['max_day'])[0] - 2))].count())
    feature['2_action_count_in_3_days'] = int (row.action_type[(row.action_type==2)&(row.day >= (list(row['max_day'])[0] - 2))].count())
    feature['3_action_count_in_3_days'] = int (row.action_type[(row.action_type==3)&(row.day >= (list(row['max_day'])[0] - 2))].count())
    feature['4_action_count_in_3_days'] = int (row.action_type[(row.action_type==4)&(row.day >= (list(row['max_day'])[0] - 2))].count())
    feature['5_action_count_in_3_days'] = int (row.action_type[(row.action_type==5)&(row.day >= (list(row['max_day'])[0] - 2))].count())
    
    feature['0_action_count_in_5_days'] = int (row.action_type[(row.action_type==0)&(row.day >= (list(row['max_day'])[0] - 4))].count())
    feature['1_action_count_in_5_days'] = int (row.action_type[(row.action_type==1)&(row.day >= (list(row['max_day'])[0] - 4))].count())
    feature['2_action_count_in_5_days'] = int (row.action_type[(row.action_type==2)&(row.day >= (list(row['max_day'])[0] - 4))].count())
    feature['3_action_count_in_5_days'] = int (row.action_type[(row.action_type==3)&(row.day >= (list(row['max_day'])[0] - 4))].count())
    feature['4_action_count_in_5_days'] = int (row.action_type[(row.action_type==4)&(row.day >= (list(row['max_day'])[0] - 4))].count())
    feature['5_action_count_in_5_days'] = int (row.action_type[(row.action_type==5)&(row.day >= (list(row['max_day'])[0] - 4))].count())
    
    feature['0_action_count_in_7_days'] = int (row.action_type[(row.action_type==0)&(row.day >= (list(row['max_day'])[0] - 6))].count())
    feature['1_action_count_in_7_days'] = int (row.action_type[(row.action_type==1)&(row.day >= (list(row['max_day'])[0] - 6))].count())
    feature['2_action_count_in_7_days'] = int (row.action_type[(row.action_type==2)&(row.day >= (list(row['max_day'])[0] - 6))].count())
    feature['3_action_count_in_7_days'] = int (row.action_type[(row.action_type==3)&(row.day >= (list(row['max_day'])[0] - 6))].count())
    feature['4_action_count_in_7_days'] = int (row.action_type[(row.action_type==4)&(row.day >= (list(row['max_day'])[0] - 6))].count())
    feature['5_action_count_in_7_days'] = int (row.action_type[(row.action_type==5)&(row.day >= (list(row['max_day'])[0] - 6))].count())
    feature['0_action_count_div_sum'] = float(feature['0_action_count']/feature['activity_count'])
    feature['1_action_count_div_sum'] = (feature['1_action_count'] + 1)/(feature['activity_count'] + 1)
    feature['2_action_count_div_sum'] = (feature['2_action_count'] + 1)/(feature['activity_count'] + 1)
    feature['3_action_count_div_sum'] = (feature['3_action_count'] + 1)/(feature['activity_count'] + 1)
    feature['4_action_count_div_sum'] = (feature['4_action_count'] + 1)/(feature['activity_count'] + 1)
    feature['5_action_count_div_sum'] = (feature['5_action_count'] + 1)/(feature['activity_count'] + 1)
    feature['max_act_times_per_day'] = int (row.groupby('day').count()['user_id'].max())
    feature['max_hot_vid'] = float(row.vid_hotness.max())
    feature['max_hot_auth'] = float(row.auth_hotness.max())
    feature['mean_hot_vid'] = float(row.vid_hotness.mean())
    feature['mean_hot_auth'] = float(row.auth_hotness.mean())
    feature['std_hot_vid'] = float(row.vid_hotness.std())
    feature['kur_hot_vid'] = float(row.vid_hotness.kurt())
    feature['min_hot_vid'] = float(row.vid_hotness.min())
    feature['min_hot_auth'] = float(row.auth_hotness.min())
    feature['sum_hot_vid'] = float(row.vid_hotness.sum())
    feature['sum_hot_auth'] = float(row.auth_hotness.sum())
    feature['std_hot_auth'] = float(row.auth_hotness.std())
    feature['kur_hot_auth'] = float(row.auth_hotness.kurt())
    feature['norm_max_hot_vid'] = float(row.video_norm_hot.max())
    feature['norm_max_hot_auth'] = float(row.author_norm_hot.max())
    feature['norm_mean_hot_vid'] = float(row.video_norm_hot.mean())
    feature['norm_mean_hot_auth'] = float(row.author_norm_hot.mean())
    feature['norm_std_hot_vid'] = float(row.video_norm_hot.std())
    feature['norm_kur_hot_vid'] = float(row.video_norm_hot.kurt())
    feature['norm_min_hot_vid'] = float(row.video_norm_hot.min())
    feature['norm_min_hot_auth'] = float(row.author_norm_hot.min())
    feature['norm_sum_hot_vid'] = float(row.video_norm_hot.sum())
    feature['norm_sum_hot_auth'] = float(row.author_norm_hot.sum())
    feature['norm_std_hot_auth'] = float(row.author_norm_hot.std())
    feature['norm_kur_hot_auth'] = float(row.author_norm_hot.kurt())
    if row.day.count() > 1:
        a = np.array(row.day)[:-1]
        b = np.array(row.day)[1:]
        tmp = pd.DataFrame(b-a)
        feature['activity_day_diff_mean'] = float (tmp.mean())
        feature['activity_day_diff_max']  = int (tmp.max())
        feature['activity_day_diff_min']  = int (tmp.min())
        feature['activity_day_diff_std'] = float (tmp.std())
        feature['activity_day_diff_var'] = float (tmp.var())
        feature['activity_day_diff_ske'] = float (tmp.skew())
        feature['activity_day_diff_kur'] = float (tmp.kurt())
    else:
        feature['activity_day_diff_mean'] = np.nan
        feature['activity_day_diff_max']  = np.nan
        feature['activity_day_diff_min']  = np.nan
        feature['activity_day_diff_std'] = np.nan
        feature['activity_day_diff_var'] = np.nan
        feature['activity_day_diff_ske'] = np.nan
        feature['activity_day_diff_kur'] = np.nan
    return feature  

def deal_feature(path, user_id):
    reg = pd.read_csv(path + register)
    cre = pd.read_csv(path + create)
    lau = pd.read_csv(path + launch)
    act = pd.read_csv(path + activity)
    feature = pd.DataFrame()
    feature['user_id'] = user_id
    
    reg_day = pd.DataFrame()
    reg_day['user_id'] = reg.user_id
    reg_day['register_day'] = reg.register_day
    
    
    cre = pd.merge(cre,reg_day,on='user_id', how='left')
    cre['max_day'] = np.max(reg['register_day'])
    cre_feature = cre.groupby('user_id', sort = True).apply(get_create_feature)
    feature = pd.merge(feature, pd.DataFrame(cre_feature), on='user_id', how='left')
    print('create表特征提取完毕')
    
    feature = pd.merge(feature, reg, on='user_id', how='left')
    feature['register_day_cut_max_day'] = (np.max(reg['register_day']) - feature.register_day)
    print('register表特征提取完毕')
    
    lau = pd.merge(lau,reg_day,on='user_id', how='left')
    lau['max_day'] = np.max(reg['register_day'])
    lau_feature = lau.groupby('user_id', sort = True).apply(get_launch_feature)
    feature = pd.merge(feature, pd.DataFrame(lau_feature), on='user_id', how='left')
    print('launch表特征提取完毕')
    
    act = pd.merge(act,reg_day,on='user_id', how='left')
    act['max_day'] = np.max(reg['register_day'])
    act_feature = act.groupby('user_id', sort = True).apply(get_activity_feature)
    feature = pd.merge(feature, pd.DataFrame(act_feature), on='user_id', how='left')
    print('activity表特征提取完毕')
    
    aut = pd.DataFrame(act.author_id)
    aut['watched_times_recent_days'] = 1
    a = aut.groupby('author_id').count()
    a_norm = (a.watched_times_recent_days - a.watched_times_recent_days.min()) / (a.watched_times_recent_days.max() - a.watched_times_recent_days.min())
    a['norm_watched_times_recent_days'] = a_norm
    a.reset_index(inplace=True)
    a.rename(columns={'author_id':'user_id'}, inplace=True)
    feature = pd.merge(feature, a, on='user_id', how='left')
    
    auth_3 = pd.DataFrame(act.author_id[act.day >= int (np.max(reg['register_day']) - 2)])
    auth_3['watched_times_recent_3_days'] = 1
    auth3 = auth_3.groupby('author_id').count()
    auth3.reset_index(inplace=True)
    auth3.rename(columns={'author_id':'user_id'}, inplace=True)
    feature = pd.merge(feature, auth3, on='user_id', how='left')
    
    auth_5 = pd.DataFrame(act.author_id[act.day >= int (np.max(reg['register_day']) - 4)])
    auth_5['watched_times_recent_5_days'] = 1
    auth5 = auth_5.groupby('author_id').count()
    auth5.reset_index(inplace=True)
    auth5.rename(columns={'author_id':'user_id'}, inplace=True)
    feature = pd.merge(feature, auth5, on='user_id', how='left')
    
    auth_7 = pd.DataFrame(act.author_id[act.day >= int (np.max(reg['register_day']) - 6)])
    #下面是后来补加的特征
    auth_7['watched_times_recent_7_days'] = 1
    auth7 = auth_7.groupby('author_id').count()
    auth7.reset_index(inplace=True)
    auth7.rename(columns={'author_id':'user_id'}, inplace=True)
    feature = pd.merge(feature, auth7, on='user_id', how='left')
    feature['watched_times_recent_5_3_days'] = feature['watched_times_recent_5_days'] - feature['watched_times_recent_3_days']
    feature['watched_times_recent_7_5_days'] = feature['watched_times_recent_7_days'] - feature['watched_times_recent_5_days']
    feature['create_count_in_last_3_5_day'] = feature['create_count_in_last_5_day'] - feature['create_count_in_last_3_day']
    feature['create_count_in_last_5_7_day'] = feature['create_count_in_last_7_day'] - feature['create_count_in_last_5_day']
    feature['create_count_in_last_7_9_day'] = feature['create_count_in_last_9_day'] - feature['create_count_in_last_7_day']
    feature['launch_count_in_3_5_day'] = feature['launch_count_in_last_5_day'] - feature['launch_count_in_last_3_day']
    feature['launch_count_in_5_7_day'] = feature['launch_count_in_last_7_day'] - feature['launch_count_in_last_5_day']
    feature['launch_count_in_7_9_day'] = feature['launch_count_in_last_9_day'] - feature['launch_count_in_last_7_day']
    feature['act_count_in_last_3_5_day'] = feature['act_count_in_last_5_day'] - feature['act_count_in_last_3_day']
    feature['act_count_in_last_5_7_day'] = feature['act_count_in_last_7_day'] - feature['act_count_in_last_5_day']
    feature['act_count_in_last_7_9_day'] = feature['act_count_in_last_9_day'] - feature['act_count_in_last_7_day']
    return feature

def get_data_feature():
    one_train_data = get_train_label(one_dataSet_train_path, one_dataSet_test_path)
    one_feature = deal_feature(one_dataSet_train_path, one_train_data['user_id'])
    one_feature['label'] = one_train_data['label']
    one_feature.to_csv('data_one_plus.csv', index=False)
    print('第一组训练数据特征值提取完毕并保存')

    two_train_data = get_train_label(two_dataSet_train_path, two_dataSet_test_path)
    two_feature = deal_feature(two_dataSet_train_path, two_train_data['user_id'])
    two_feature['label'] = two_train_data['label']
    two_feature.to_csv('data_two_plus.csv', index=False)
    print('第二组训练数据特征值提取完毕')
    
    train_feature = pd.concat([one_feature, two_feature])
    train_feature.to_csv('data/train_and_test/train_plus.csv', index = False)
    print('训练数据存储完毕')
    
    test_data = get_test(three_dataSet_train_path)
    test_feature = deal_feature(three_dataSet_train_path, test_data['user_id'])
    test_feature.to_csv('data/train_and_test/test_plus.csv', index=False)
    print('测试数据存储完毕')
get_data_feature()
```

有点长哈,感兴趣的可以移驾到本人的github,或许jupyter notebook的格式看起来更好一点

#### 三. 模型预测

这次比赛使用了lightgbm,好处是速度快,不用归一化,不用处理缺失值,我也试过XGboost,线上提交了一次,效果不太好就没再用了.由于lightgbm我也是第一次接触,对于参数的把控并不好,所以这里的参数应该是没有什么参考意义的.,这里只上部分代码:

```python
print('开始处理特征......')
# train_path = 'data/train_and_test/train_plus.csv'
# test_path = 'data/train_and_test/test_plus.csv'
# train = pd.read_csv(train_path)
# test = pd.read_csv(test_path)
used_feature = [
'create_count_recent_days',
#  'create_count_in_last_3_day',
#  'create_count_in_last_5_day',
#  'create_count_in_last_7_day',
#  'create_count_in_last_9_day',
 'create_day_mean',
#  'create_day_max',
#  'create_day_min',
 'create_day_max_sub_min',
 'create_day_std',
 'create_day_var',
 'create_day_skew',
 'create_day_kur',
 'last_day_cut_max_day',
 'first_day_cut_register_day',
 'max_create_times_one_day',
 'create_day_diff_mean',
 'create_day_diff_max',
 'create_day_diff_min',
 'create_day_diff_std',
 'create_day_diff_var',
 'register_day',
 'register_type',
 'device_type',
 'register_day_cut_max_day',
 'launch_count_recent_days',
 'reg_now_diff',
#  'launch_count_in_last_3_day',
#  'launch_count_in_last_5_day',
#  'launch_count_in_last_7_day',
#  'launch_count_in_last_9_day',
#  'launch_day_mean',
#  'launch_day_max',
#  'launch_day_min',
 'launch_day_max_sub_min',
 'launch_day_std',
 'launch_day_var',
 'launch_day_kur',
 'launch_day_skew',
 'last_launch_day_cut_max_day',
 'first_launch_day_cut_register_day',
#  'launch_mean_cut_max_day',
 'max_launch_times_one_day',
 'launch_day_diff_mean',
 'launch_day_diff_max',
 'launch_day_diff_min',
 'launch_day_diff_std',
 'launch_day_diff_var',
#  'act_count_in_last_3_day',
#  'act_count_in_last_5_day',
#  'act_count_in_last_7_day',
#  'act_count_in_last_9_day',
 'activity_count',
 'activity_day_mean',
#  'activity_day_max',
#  'activity_day_min',
 'activity_day_std',
 'activity_day_var',
 'activity_day_ske',
 'activity_day_kur',
 'activity_day_max_cut_min',
 'activity_day_cut_max_day',
 'activity_day_cut_register_day',
 'act_sub_register',
 '0_page_count',
 '1_page_count',
 '2_page_count',
 '3_page_count',
 '4_page_count',
#  '0_page_count_in_3_days',
#  '1_page_count_in_3_days',
#  '2_page_count_in_3_days',
#  '3_page_count_in_3_days',
#  '4_page_count_in_3_days',
#  '0_page_count_in_5_days',
#  '1_page_count_in_5_days',
#  '2_page_count_in_5_days',
#  '3_page_count_in_5_days',
#  '4_page_count_in_5_days',
#  '0_page_count_in_7_days',
#  '1_page_count_in_7_days',
#  '2_page_count_in_7_days',
#  '3_page_count_in_7_days',
#  '4_page_count_in_7_days',
 '0_page_count_div_sum',
 '1_page_count_div_sum',
 '2_page_count_div_sum',
 '3_page_count_div_sum',
 '4_page_count_div_sum',
 '0_action_count',
 '1_action_count',
 '2_action_count',
 '3_action_count',
 '4_action_count',
 '5_action_count',
#  '0_action_count_in_3_days',
#  '1_action_count_in_3_days',
#  '2_action_count_in_3_days',
#  '3_action_count_in_3_days',
#  '4_action_count_in_3_days',
#  '5_action_count_in_3_days',
#  '0_action_count_in_5_days',
#  '1_action_count_in_5_days',
#  '2_action_count_in_5_days',
#  '3_action_count_in_5_days',
#  '4_action_count_in_5_days',
#  '5_action_count_in_5_days',
#  '0_action_count_in_7_days',
#  '1_action_count_in_7_days',
#  '2_action_count_in_7_days',
#  '3_action_count_in_7_days',
#  '4_action_count_in_7_days',
#  '5_action_count_in_7_days',
 '0_action_count_div_sum',
 '1_action_count_div_sum',
 '2_action_count_div_sum',
 '3_action_count_div_sum',
 '4_action_count_div_sum',
 '5_action_count_div_sum',
 'max_act_times_per_day',
 'max_hot_vid',
 'max_hot_auth',
 'mean_hot_vid',
 'mean_hot_auth',
#  'std_hot_vid',
#  'kur_hot_vid',
#  'min_hot_vid',
#  'min_hot_auth',
#  'sum_hot_vid',
#  'sum_hot_auth',
#  'std_hot_auth',
#  'kur_hot_auth',
 'norm_max_hot_vid',
 'norm_max_hot_auth',
 'norm_mean_hot_vid',
 'norm_mean_hot_auth',
 'norm_std_hot_vid',
 'norm_kur_hot_vid',
 'norm_min_hot_vid',
 'norm_min_hot_auth',
 'norm_sum_hot_vid',
 'norm_sum_hot_auth',
 'norm_std_hot_auth',
 'norm_kur_hot_auth',
 'activity_day_diff_mean',
 'activity_day_diff_max',
 'activity_day_diff_min',
 'activity_day_diff_std',
 'activity_day_diff_var',
 'activity_day_diff_ske',
 'activity_day_diff_kur',
 'watched_times_recent_days',
 'norm_watched_times_recent_days',
#  'watched_times_recent_3_days',
#  'watched_times_recent_5_days',
#  'watched_times_recent_7_days',
#  'watched_times_recent_5_3_days',
#  'watched_times_recent_7_5_days',
#  'create_count_in_last_3_5_day',
#  'create_count_in_last_5_7_day',
#  'create_count_in_last_7_9_day',
#  'launch_count_in_3_5_day',
#  'launch_count_in_5_7_day',
#  'launch_count_in_7_9_day',
#  'act_count_in_last_3_5_day',
#  'act_count_in_last_5_7_day',
#  'act_count_in_last_7_9_day',
]
used_feature = np.array(used_feature)
# print(used_feature)
# importance_feature = [2, 3, 6, 7, 11, 13, 14, 21, 22, 23, 25, 27, 28, 29, 30, 31, 32, 33, 35, 36, 37, 41, 42, 43, 44, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 58, 62, 63, 64, 66, 67, 68, 69, 70, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 93, 94, 95, 99, 101, 105, 106, 108, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 125, 126, 127, 128, 129, 131, 132, 133, 134, 135, 138, 139, 140, 141, 142, 144, 145, 147, 148, 151, 152, 153, 162, 163]
# used_feature = used_feature[np.array(importance_feature)]
print(used_feature)
# train_feature = train[used_feature]
# test_feature = test[used_feature]
# label = train['label']

train = pd.read_csv('data_one_plus.csv')
test2 = pd.read_csv('data_two_plus.csv')
X_train = train[used_feature]
Y_train = train['label']

X_test = test2[used_feature]
Y_test= test2['label']

# X_test1 = test2[used_feature].head(10000)
# Y_test1 = test2['label'].head(10000)
# X_test2 = test2[used_feature].tail(27332)
# Y_test2 = test2['label'].tail(27332)


# 切分训练
# X_train, X_test, Y_train, Y_test = model_selection.train_test_split(train_feature, label, test_size=0.2,random_state=104)
# train_feature = X_train
# label = Y_train

print('特征处理完毕......')

print('载入数据......')
# lgb_train = lgb.Dataset(train_feature, label)
# lgb_eval = lgb.Dataset(X_test, Y_test, reference=lgb_train,free_raw_data=False)
lgb_train = lgb.Dataset(X_train, Y_train)
lgb_eval = lgb.Dataset(X_test, Y_test, reference=lgb_train,free_raw_data=False)
### 设置初始参数--不含交叉验证参数
print('设置参数')
params = {
    'boosting_type': 'gbdt',
    'objective': 'binary',
    'metric': {'auc','binary_logloss'},
    'num_leaves':12,
#     'random_state' : 1053,
    'early_stopping_round':10,
#     'learning_rate': 0.1,
#     'min_data_in_leaf':300,
#     'bagging_fraction':1,
#     'lambda_l1': 1,    
#     'lambda_l2': 0.01,  # 越小l2正则程度越高  
}
gbm = lgb.train(params,
                lgb_train,
                num_boost_round=1118,
                valid_sets=lgb_eval,
                
                )
gbm.save_model('model/lgb_model.txt')

temp = gbm.predict(X_test)
temp[temp>=0.44]=1
temp[temp<0.44]=0
print('结果：' + str(sklearn.metrics.f1_score(Y_test, temp)))
# print('召回率结果：' + str(sklearn.metrics.recall_score(Y_test, temp)))
# print('精确率结果：' + str(sklearn.metrics.precision_score(Y_test, temp)))
print('特征重要性：'+ str(list(gbm.feature_importance())))

#交叉验证调参
params = {
          'boosting_type': 'gbdt',
          'objective': 'binary',
          'metric': 'binary_logloss',
          }

### 交叉验证(调参)
print('交叉验证')
min_merror = float('Inf')
best_params = {}

# 准确率
print("调参1：提高准确率")
for num_leaves in range(4,100,2):
    for max_depth in range(2,8,1):
        params['num_leaves'] = num_leaves
        params['max_depth'] = max_depth

        cv_results = lgb.cv(
                            params,
                            lgb_train,
                            seed=1012,
                            nfold=3,
                            metrics=['binary_error'],
                            early_stopping_rounds=10,
                            verbose_eval=True
                            )

        mean_merror = pd.Series(cv_results['binary_error-mean']).min()
        boost_rounds = pd.Series(cv_results['binary_error-mean']).argmin()

        if mean_merror < min_merror:
            min_merror = mean_merror
            best_params['num_leaves'] = num_leaves
            best_params['max_depth'] = max_depth

params['num_leaves'] = best_params['num_leaves']
params['max_depth'] = best_params['max_depth']
```

#### 四.赛后总结

参加这个比赛对我个人来说有重大意义,毕竟是第一个大部分工作都由自己完成的比赛,虽然前期借鉴了群友的数据划分的策略,但是从后面的提特征和调参是实打实自己干的,这个比赛有些槽点,就是用f1作为评分准则,使得不能单纯以提高准确率为目标,最后还得考量提交的用户的数量,就变成了调试阈值的比赛.





