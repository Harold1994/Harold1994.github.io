---
title: 春招实习遇到的问题
date: 2019-03-23 11:24:59
tags: [Java, 操作系统, 大数据]
---

#### 一、操作系统

- 操作系统中进程中的线程会共享栈吗？

  一个进程实际上是有多个线程的执行单元组成，每个线程都运行在进程的上下文中，并共享同样的代码和全局数据。

  先来看看栈是从哪里来的：

  虚拟存储器是一个抽象概念，它为每个进程提供了假象，即每个进程都在独占的使用主存。每个进程看到的是一致的存储器，称为虚拟地址空间。下图是 Linux 进程的虚拟地址空间（其他Unix 系统设计类似）：

  <!-- more-->

  ![1120165-20170911080525000-869430973.png](https://i.loli.net/2019/03/22/5c948bd25a761.png)

  我们从最低的地址，逐步向上介绍上面出现的区域名词。

  　　①、程序代码和数据：对于所有进程来说，代码是从同一固定地址开始的，分别为0x08048000（32位）以及0x00400000（64位），紧接着是全局变量相对应的数据位置。

  　　②、堆：代码和数据区后紧随的是运行时堆。代码和数据区是在进程一开始运行时就规定了大小，而当调用malloc和free这样的 C 标准库函数 时，堆可以在运行时动态的扩展和收缩。

  　　③、共享库：存放像C标准库和数据库这样的代码和数据的区域。

  　　④、栈：位于用户虚拟地址空间顶部，编译器用它来实现函数调用，用户栈在程序执行期间可以动态的扩展和收缩。当我们调用一个函数时，栈会增长；从一个函数返回时，栈会收缩。

  　　⑤、内核虚拟存储器：内核总是驻留在内存中，是操作系统的一部分，不允许应用程序读写这个区域的内容或者直接调用内核代码定义的函数。

   　　关于所有进程代码是从同一固定地址开始的，这里我们做个验证，我们在 Linux 64位系统上，在存放hello.c程序的目录下输入如下命令：运行前面写好的 hello.c 程序，并查看地址信息

  > 可见，位于虚拟地址空间顶部的是用户栈，编译器用它来实现函数调用，用户栈再执行期间可以动态的扩展和伸缩，每次调用一个函数时，栈就会增长，从一个函数返回时，栈就会收缩。

  一组并发线程运行在一个进程的上下文中，**所有线程都有它独立的线程上下文，包括线程ID、栈、栈指针、程序计数器、条件码和通用目的寄存器值**，每个线程和其他线程共享进程上下文的其他部分，包括整个用户虚拟地址空间，线程也共享相同的打开文件的集合。

  各自独立的线程栈被保存在虚拟地址空间的栈区域中，并且通常是被相应的线程独立访问的，不过有的时候线程可以读取到另一个线程的栈中的数据，例如一个线程以某种方式得到一个指向其他线程栈的指针。

#### 二、Java

- Executor实现，自己怎么实现

  > 阿里发布的 Java开发手册中强制线程池不允许使用 Executors 去创建，而是通过 ThreadPoolExecutor 的方式，这样的处理方式让写的同学更加明确线程池的运行规则，规避资源耗尽的风险

  线程池体系：

  ![屏幕快照 2019-03-22 下午5.03.14.png](https://i.loli.net/2019/03/22/5c94a57c740d8.png)

  |-Java.util.concurrent.Executor 负责线程的使用与调度的根接口

  |-ExecutorService:Executor的子接口，线程池的主要接口

  |-AbstractExecutorService:实现了ExecutorService接口，基本实现了ExecutorService其中声明的所有方法，另有添加其他方法

  |-ThreadPoolExecutor:继承了AbstractExecutorService，主要的常用实现类

  |-ScheduledExecutorService:继承了ExecutorService，负责线程调度的接口

  |-ScheduledThreadPoolExecutor:继承了ThreadPoolExecutor同时实现了ScheduledExecutorService

- 自己实现线程池可以通过ThreadPollExecutor来创建，重要的是调节传入的参数：

  ```java
  public ThreadPoolExecutor(int corePoolSize,
                            int maximumPoolSize,
                            long keepAliveTime,
                            TimeUnit unit,
                            BlockingQueue<Runnable> workQueue,
                            ThreadFactory threadFactory,
                            RejectedExecutionHandler handler) 
  ```

  - corePoolSize: 线程池核心池的大小
  - maximumPoolSize:线程池的最大线程数
  - keepAliveTime ：当线程数大于核心时，此为终止前多余的空闲线程等待新任务的最长时间
  - unit： keepAliveTime 的时间单位
  - workQueue：用来储存等待执行任务的队列
  - threadFactory ： 线程工厂。
  - handler ：拒绝策略

  1. **线程池大小**
     线程池有两个线程数的设置，一个为核心池线程数，一个为最大线程数。
     在创建了线程池后，默认情况下，线程池中并没有任何线程，等到有任务来才创建线程去执行任务，除非调用了prestartAllCoreThreads()或者prestartCoreThread()方法
     当创建的线程数等于 corePoolSize 时，会加入设置的阻塞队列。当队列满时，会创建线程执行任务直到线程池中的数量等于maximumPoolSize。

  2. **工作队列**

     在线程池中，任务请求会在一个由Executor管理的Runnable队列中等待，而不是像线程那样去竞争cpu资源，通过一个Runnable和一个链表节点来表现一个等待中的任务，开销比用线程表示低很多，但如果客户端提交给服务器请求的速率超过了服务器的处理速率，那么仍可能会耗尽资源。

     ThreadPoolExecutor允许提供一个BlockingQueue来保存等待执行的任务，基本任务队列有三种：

     - 无界队列：newFixedThreadPool和newSingleThreadExecutor默认使用LinkedBlockingQueue，如果所有工作者线程都忙碌，那么任务将在队列中等候，缺点是队列可能无限制增加，造成资源耗尽。
     - 有界队列：如ArrayBlockingQueue、有界的LinkedBlockingQueue、PriorityBlockingQueue。有界队列有助于避免资源耗尽，但是如果队列满了，新来的任务的处理成为了问题，**饱和策略**可以解决此类问题。
     - 同步移交（SynchronousQueue）:直接将任务从生产者移交给工作者线程，SynchronousQueue是一种线程之间的移交机制，要将一个元素放入SynchronousQueue，必须有另外一个线程正在等待接受这个元素。如果没有线程正在等待并且线程池当前大小小于最大值，SynchronousQueue将创建一个新线程，否则根据饱和策略，这个任务将被拒绝。newCachedThreadPool工厂方法中使用了SynchronousQueue。

     > 只有当任务相互独立时，为线程池工作队列设置界限才是合理的。若任务之间存在依赖关系，那么有界队列可能导致“饥饿”死锁问题，此时应该使用无界线程池，如newCachedThreadPool.

     ArrayBlockingQueue ：一个由数组结构组成的有界阻塞队列。
     LinkedBlockingQueue ：一个由链表结构组成的有界阻塞队列。
     PriorityBlockingQueue ：一个支持优先级排序的无界阻塞队列。
     DelayQueue： 一个使用优先级队列实现的无界阻塞队列。
     SynchronousQueue： 一个不存储元素的阻塞队列。
     LinkedTransferQueue： 一个由链表结构组成的无界阻塞队列。

     LinkedBlockingDeque： 一个由链表结构组成的双向阻塞队列

  3. #### 拒绝策略策略

     当有界队列被填满后，饱和策略开始发挥作用。ThreadPoolExecutor饱和策略可以通过setRejectedExecutionHandler来修改。（若某个任务被提交到已经关闭的Executor，也会用到饱和策略）。

     - **中止（Abort）**：默认的饱和策略，该策略将会抛出未受检查的RejectedEcecutionExeption，调用者捕获后可以自行处理
     - **抛弃（Discard）**：该策略会悄悄抛弃该任务
     - **抛弃最旧的（Discard-Oldest）**：抛弃下一个将被执行的任务，然后尝试重新提交新任务。
     - **调用者运行（Caller-Runs）**：实现了一种调节机制，既不会抛弃任务，也不会抛出异常，而是将任务回退到调用者，由调用者执行，从而降低新任务的流量。

  说明：Executors 各个方法的弊端：
  1）newFixedThreadPool 和 newSingleThreadExecutor:
  主要问题是堆积的请求处理队列可能会耗费非常大的内存，甚至 OOM。
  2）newCachedThreadPool 和 newScheduledThreadPool:
  主要问题是线程数最大数是 Integer.MAX_VALUE，可能会创建数量非常多的线程，甚至 OOM。

#### 三、大数据组件

- Flume与Logstash的区别

  **Logstash:** 

  1. 插件式组织方式，易于扩展和控制
  2. 数据源多样不仅限于日志文件，数据处理操作更丰富，可自定义（过滤，匹配过滤，转变，解析......）
  3. 可同时监控多个数据源（input插件多样），同时也可将处理过的数据同时有不同多种输出（如stdout到控制台，同时存入elasticsearch）
  4. 安装简单，使用简单，结构也简单，所有操作全在配置文件设定，运行调用配置文件即可
  5. 管道式的dataSource——input plugin——filter plugin——output plugin——dataDestination
  6. 有logstash web界面，可搜索日志
  7. 有一整套的EKL日志追踪技术栈，可收集处理（logstash），存储管理搜索（elasticsearch），图形显示分析（kibana）
  8. 做到更好的实时监控（插件设置时间间隔属性，对监控的数据源检查更新）

  **Flume (1.x  flume-ng）**

  1.分布式的可靠的可用的系统，高效的从不同数据源收集聚合迁移大量数据到一个集中的数据存储

  2.安装部署比较logstash复杂

  3.同样以配置文件为中心   提供了JavaAPI

  4.是一个完整的基于插件的架构 有独立开发的第三方插件

  5.三层架构：source  channel  sink

  Flume使用基于事务的数据传递方式来保证事件传递的可靠性。Source和Sink被封装进一个事务。事件被存放在Channel中直到该事件被处理，Channel中的事件才会被移除。这是Flume提供的点到点的可靠机制。

  从多级流来看，前一个agent的sink和后一个agent的source同样有它们的事务来保障数据的可靠性。

  > 两者最初的设计目的就不太一样。Flume本身最初设计的目的是为了把数据传入HDFS中（并不是为了采集日志而设计，这和Logstash有根本的区别），所以理所应当侧重于数据的传输，程序员要非常清楚整个数据的路由，并且比Logstash还多了一个可靠性策略，上文中的channel就是用于持久化目的，数据除非确认传输到下一位置了，否则不会删除，这一步是通过事务来控制的，这样的设计使得可靠性非常好。相反，Logstash则明显侧重对数据的预处理，因为日志的字段需要大量的预处理，为解析做铺垫。

- Flume与Kafka的区别

  Flume ：管道 ----个人认为比较适合有多个生产者场景，或者有写入Hbase、HDFS和kafka需求的场景。

  Kafka ：消息队列-----由于Kafka是Pull模式，因此适合有多个消费者的场景。

- 为什么用这样的数据管道

  Spark Streaming+Flume+Kafka是大数据准实时数据采集的最为可靠并且也是最常用的方案

  整个过程就是，Flume会[实时监控](https://www.baidu.com/s?wd=%E5%AE%9E%E6%97%B6%E7%9B%91%E6%8E%A7&tn=24004469_oem_dg&rsv_dl=gh_pl_sl_csd)写入日志的磁盘，只要有新的日志写入，Flume就会将日志以消息的形式传递给Kafka，然后Spark Streaming实时消费消息传入ES,

  Kafka是一个消息系统，Flume收集的日志可以移动到Kafka消息队列中，然后就可以被多处消费了，而且可以保证不丢失数据,通过这套架构，收集到的日志可以及时被Flume发现传到Kafka，通过Kafka我们可以把日志用到各个地方，同一份日志可以存入Hdfs中，也可以离线进行分析，还可以实时计算，而且可以保证安全性，基本可以达到实时的要求

