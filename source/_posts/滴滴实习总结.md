---
title: 滴滴实习总结
date: 2018-10-08 16:28:00
tags: [大数据, 算法]
---

How time flies，在滴滴的实习经历已然到了尾声，在公司呆了两个半月，收获了很多东西，除了是技术上，还有一起工作的组内的优秀的同事们。这篇帖子作为自己的这段时间在技术上和方法上的积累的总结，比较零散，想到哪写到哪吧。

我做的工作是券核销模型的构建，在以后的定价策略和预算估计上会用到，估计是基础模型之一吧，这个模型大约有三分之二的时间用在了特征的构造上，构造特征主要用到的工具就是Hive和shell脚本，偶尔会用到python，就从它们总结起：

<!-- more-->

#### 一、Hive的知识点

* 在创建表的时候可以指定的东西不仅仅只有`ROW FORMAT DELIMITED FIELDS TERMINATED BY ...`
  * ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.orc.OrcSerde': 指定序列化格式

  * STORED AS INPUTFORMAT  'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat' ：Hive中，默认使用的是TextInputFormat，一行表示一条记录。在每条记录(一行中)，默认使用^A分割各个字段

  * LOCATION '$DIR'：创建内部表时没有指定location，就会在/user/hive/warehouse/下新建一个表目录

  * TBLPROPERTIES ('LEVEL'='1', 'TTL'='360' )：TBLPROPERTIES允许开发者定义一些自己的键值对信息


> SerDe is a short name for “Serializer and Deserializer.”
> Hive uses SerDe (and !FileFormat) to read and write table rows.
> HDFS files –> InputFileFormat –> <key, value> –> Deserializer –> Row object
> Row object –> Serializer –> <key, value> –> OutputFileFormat –> HDFS files

> ORC的全称是(Optimized Row Columnar)，ORC文件格式是一种Hadoop生态圈中的列式存储格式，它的产生早在2013年初，最初产生自Apache Hive，用于降低Hadoop数据存储空间和加速Hive查询速度。和Parquet类似，它并不是一个单纯的列式存储格式，仍然是首先根据行组分割整个表，在每一个行组内进行按列存储。ORC文件是自描述的，它的元数据使用Protocol Buffers序列化，并且文件中的数据尽可能的压缩以降低存储空间的消耗，

* CONCAT_WS(separator, str1, str2,...):第一个参数剩余参数间的分隔符。分隔符可以是与剩余参数一样的字符串。如果分隔符是 NULL，返回值也将为 NULL。这个函数会跳过分隔符参数后的任何 NULL 和空字符串。分隔符将被加到被连接的字符串之间

* datediff(string enddate,string startdate):返回结束日期减去开始日期的天数。 

* date_add(string startdate, intdays):返回开始日期startdate增加days天后的日期。 

* date_sub (string startdate,int days):返回开始日期startdate减少days天后的日期。 

* case when then else end可以用在count(),sum(),avg()等统计函数中，e.g

  `select ... ,sum(case when label=1 then 1 else 0 end) as pos_count,...`

  `select (case when A.B is null then 0 else 1 end) as label ...`

* 在hive sql中查找当前时间:CURRENT_DATE,CURRENT_TIMESTAMP, from_unixtime(unix_timestamp())

* union用于联合多个select语句的结果集，合并为一个独立的结果集。当前只支持UNION ALL(bag union)。不能消除重复行，每个select语句返回的列的数量和名字必须一样，否则会抛出语法错误。

  `select_statement UNION ALL select_statement UNION ALL select_statement...`

* HIVE直接读入json的函数有两个：

  （1）get_json_object(string json_string, string path)

  返回值: string  

  说明：解析json的字符串json_string,返回path指定的内容。如果输入的json字符串无效，那么返回NULL。  

  举例：  

  ```sql
  hive> select  get_json_object(‘{“store”:{“fruit”:\[{"weight":8,"type":"apple"},{"weight":9,"type":"pear"}],  “bicycle”:{“price”:19.95,”color”:”red”}}, “email”:”amy@only_for_json_udf_test.net”,   “owner”:”amy” } ‘,’$.owner’) from dual;  
  ```

  结果：amy 

  这个函数每次只能返回一个数据项。

  （2）json_tuple(jsonStr, k1, k2, ...)

  参数为一组键k1，k2……和JSON字符串，返回值的元组。该方法比 `get_json_object` 高效，因为可以在一次调用中输入多个键

* Hive 0.6版本及以上支持视图（View，详见Hive的RELEASE_NOTES.txt），Hive View具有以下特点：

  1）View是逻辑视图，暂不支持物化视图（后续将在1.0.3版本以后支持）；

  2）View是只读的，不支持LOAD/INSERT/ALTER。需要改变View定义，可以是用Alter View；

  3）View内可能包含ORDER BY/LIMIT语句，假如一个针对View的查询也包含这些语句， 则View中的语句优先级高；

  4）支持迭代View。

* FULL OUTER JOIN 关键字只要左表（table1）和右表（table2）其中一个表中存在匹配，则返回行.

  FULL OUTER JOIN 关键字结合了 LEFT JOIN 和 RIGHT JOIN 的结果。

  注意：FULL JOIN时候，Hive不会使用MapJoin来优化。

* Hive几种数据导出方式

  1）通过hive语句

  ```scala
  hive> insert overwrite local directory '/home/wyp/wyp'
      > select * from wyp;
  ```

  #### 这条HQL的执行需要启用Mapreduce完成，运行完这条语句之后，将会在本地文件系统的/home/wyp/wyp目录下生成文件，这个文件是Reduce产生的结果，必须写overwrite。

  2）利用Hadoop命令从hdfs下载

  `hadoop fs -get hdfs://host:port/user/hadoop/file localfile`

  如果hive表中的数据没有压缩或者采用不同的序列化机制，这样是可行的，不然的话下载下来的是乱码

* 在Hive中随机取样

  Hive支持桶表抽样和块抽样，所谓桶表指的是在创建表时使用CLUSTERED BY子句创建了桶的表。桶表抽样的语法如下：

  ```sql
  --数据块取样（Block Sampling）
  SELECT * FROM lxw1 TABLESAMPLE (50 PERCENT);
  
  --将会从表lxw1中取样30M的数据：
  SELECT * FROM lxw1 TABLESAMPLE (30M);
  
  --这种方式可以根据行数来取样，但要特别注意：
  --这里指定的行数，是在每个InputSplit中取样的行数，也就是，每个Map中都取样n ROWS。
  SELECT COUNT(1) FROM (SELECT * FROM lxw1 TABLESAMPLE (200 ROWS)) x;
  
  --分桶表取样（Sampling Bucketized Table）
  SELECT COUNT(1)
  FROM lxw1 TABLESAMPLE (BUCKET 1 OUT OF 10 ON rand());
  ```

 **Random sampling**

使用RAND()函数和LIMIT关键字来获取样例数据。使用DISTRIBUTE和SORT关键字来保证数据是随机分散到mapper和reducer的。ORDER BY RAND()语句可以获得同样的效果，但是性能没这么高。

--Syntax：

​        SELECT * FROM <Table_Name> DISTRIBUTE BY RAND() SORT BY RAND()  LIMIT <N rows to sample>;

#### 二、shell的知识点

* shell中多线程执行程序:这对于要回溯过去几天的hive表很有用

  Shell中并没有真正意义的多线程，要实现多线程可以启动多个后端进程，最大程度利用cpu性能。

  (1) 顺序执行的代码

  ```shell
  #!/bin/bash
  date
  for i in `seq 1 5`
  do
  {
      echo "sleep 5"
      sleep 5
  }
  done
  date
  ```

  (2) 并行代码

  **使用'&'+wait 实现“多进程”实现**

  ```shell
  #!/bin/bash
  date
  for i in `seq 1 5`
  do
  {
      echo "sleep 5"
      sleep 5
  } &
  done
  wait  ##等待所有子后台进程结束
  date
  ```

  (3) 对于大量处理任务如何实现启动后台进程的数量可控？

  　　简单的方法可以使用2层for/while循环实现，**每次wait内层循环的多个后台程序执行完成**。

  　　但是这种方式的问题是，如果**内层循环有“慢节点”**可能导致整个任务的执行执行时间长。

  　　更高级的实现可以看(4)

  (4) 使用命名管道(fifo)实现每次启动后台进程数量可控。 

  ​	这个我会开辟一个新博客详细说

* shell随机从文件中抽取若干行

  shuf 命令的选项：

  ```
  shuf -n5 main.txt
  -e, --echo                  ：将每个参数视为输入行
   -i, --input-range=LO-HI    ：将LO 到HI 的每个数字视为输入行
   -n, --head-count=行数       ： 最多输出指定的行数
   -o, --output=文件           ：将结果输出到指定文件而非标准输出
       --random-source=文件    ：从指定文件获得随机比特
   -z, --zero-terminated      ：以0 结束行而非新行
       --help                 ：显示此帮助信息并退出
       --version              ：显示版本信息并退出
  ```

* find命令原理：从指定的起始目录开始，递归地搜索其各个子目录，查找满足寻找条件的文件，并可以对其进行相关的操作。

  > -name：按照文件名查找文件。
  > -perm：按照文件权限来查找文件。
  > -prune：使用这一选项可以使find命令不在当前指定的目录中查找，如果同时使用-depth选项，那么-prune将被find命令忽略。
  > -user： 按照文件属主来查找文件。
  > -group：按照文件所属的组来查找文件。
  > -mtime -n +n：按照文件的更改时间来查找文件， - n表示文件更改时间距现在n天以内，+ n表示文件更改时间距现在n天以前。find命令还有- a t i m e和- c t i m e选项，但它们都和- mtime选项。
  > -nogroup：查找无有效所属组的文件，即该文件所属的组在/etc/g r o u p s中不存在。
  > -nouser：查找无有效属主的文件，即该文件的属主在/etc/passwd中不存在。
  > -newer file1 ! file2：查找更改时间比文件f i l e 1新但比文件f i l e 2旧的文件。
  > -type 查找某一类型的文件，诸如：
  > b - 块设备文件。d - 目录。c - 字符设备文件。p - 管道文件。l - 符号链接文件。f - 普通文件。
  > -size n：[c] 查找文件长度为n块的文件，带有c时表示文件长度以字节计。
  >
  > -depth：在查找文件时，首先查找当前目录中的文件，然后再在其子目录中查找。

* find命令之exec,exec解释：

  -exec  参数后面跟的是command命令，它的终止是以;为结束标志的，所以这句命令后面的分号是不可缺少的，考虑到各个系统中分号会有不同的意义，所以前面加反斜杠。{}   花括号代表前面find查找出来的文件名。

  实例：查找文件移动到指定目录  

  命令：

  `find . -name "*.log" -exec mv {} .. \;`

* shell中的date操作

  在linux 中date 有很多用法，在这里我简单介绍一下 -d这个参数的使用方法以及使用例子

  date -d //显示字符串所指的日期与时间。字符串前后必须加上双引号

  指定显示的日期格式：

  date <+时间日期格式> 例如： date +"%Y-%m-%d" // 注意 ：+ 和格式之间没有空格 2016-11-30

* linux里source、sh、bash、./有什么区别：

  在linux里，source、sh、bash、./都可以执行shell script文件，

  **1、source**

  ```
  source a.sh
  ```

  在**当前shell内**去读取、执行a.sh，而a.sh不需要有"**执行权限**"

  source命令可以简写为"."

  ```
  . a.sh
  ```

  注意：中间是**有空格**的。 

  **2、sh/bash**

  ```
  sh a.sh
  bash a.sh
  ```

  都是**打开一个subshell**去读取、执行a.sh，而a.sh不需要有"**执行权限**"

  通常在subshell里运行的脚本里设置变量，不会影响到父shell的。

  **3、./**

  **打开一个subshell**去读取、执行a.sh，但a.sh需要有"**执行权限**"

  可以用chmod +x添加执行权限

* 获取数组的长度

  ```shell
  # 取得数组元素的个数
  length=${#array_name[@]}
  # 或者
  length=${#array_name[*]}
  # 取得数组单个元素的长度
  lengthn=${#array_name[n]}
  ```

#### 三、模型上的问题

算法方面遇到的问题基本被现场解决了，包括正负样本不均衡，过拟合，xgboost参数设置等问题，这里不赘述了