---
title: Google File System阅读笔记
date: 2018-11-13 20:18:34
tags: [大数据, Hadoop, HDFS]
---

Google File System 论文的发表催生了后来的 HDFS，后者直到今天依然是开源分布式文件系统解决方案的首选。Google MapReduce 加上 Google File System 这两篇论文可谓是大数据时代的开山之作，与 Google BigTable 并称 Google 的三架马车，由此看来这几篇经典论文还是很值得我们去学习一番的。本博客是我在阅读Google File System论文时做的笔记

#### 一、概述

GFS设计思路：

* **组件失效被认为是常态事件，而不是意外事件** (应用程序 bug、操作系统的 bug、人为失误，甚至还有硬盘、内存、连接器、网络以及电源失效等造成的问题)：GFS需要集成持续的监控、错误侦测、灾难冗余以及自动恢复等机制。
* **以通常的标准衡量，GFS要存储的文件非常巨大**：设计的假设条件和参数，比如 I/O 操作和 Block 的尺寸都需要重新考虑。
* **绝大部分文件的修改是采用在文件尾部追加数据，而不是覆盖原有数据的方式**：对于这种针对海量文件的访问模式，客户端对数据块缓存是没有意义的，数据的追加操作是性能优化和原子性保证的主要考量因素。
* **应用程序和文件系统 API 的协同设计提高了整个系统的灵活性**：放松了对 GFS 一致性模型的要求，这样就减轻了文件系统对应用程序的苛刻要求，们引入了原子性的记录追加操作，从而保证多个客户端能够同时进行追加操作，不需要额外的同步操作来保证数据的一致性

<!-- more-->

系统的工作负载主要来自两种读操作：**大规模的流式读取和小规模的随机读取**，还包括许多大规模的、顺序的、数据追加方式的写操作。

GFS 提供了快照和记录追加操作。快照以很低的成本创建一个文件或者目录树的拷贝。记录追加操作允许多个客户端同时对一个文件进行数据追加操作，同时保证每个客户端的追加操作都是**原子性**的。这对于实现多路结果合并，以及“生产者-消费者”队列非常有用，多个客户端可以在不需要额外的同步锁定的情况下，同时对一个文件追加数据。

#### 二、架构

一个 GFS 集群包含一个单独的 Master 节点、多台 Chunk 服务器，并且同时被多个客户端访问。

![gfs-architecture.png](https://i.loli.net/2018/11/14/5bec1a8913e9a.png)

* GFS 存储的文件都被分割成固定大小的 Chunk。

* Chunk 服务器把 Chunk 以 Linux 文件的形式保存在本地硬盘上，并且根据指定的 Chunk 标识和字节范围来读写块数据。出于可靠性的考虑，每个块都会复制到多个块服务器上。默认使用 3 个存储复制节点。
* Master 节点管理所有的文件系统元数据。这些元数据包括名字空间、访问控制信息、文件和 Chunk 的映
  射信息、以及当前 Chunk 的位置信息。
* 客户端和 Master 节点的通信只获取元数据，所有的数据操作都是由客户端直接和 Chunk 服务器进行交互的。

* 无论是客户端还是 Chunk 服务器都不需要缓存文件数据：对于客户端大部分程序要么以流的方式读取一个巨大文件，要么工作集太大根本无法被缓存。对于Chunk 服务器，Chunk 以本地文件的方式保存，Linux 操作系统的文件系统缓存会把经常访问的数据缓存在内存中。

* 客户端并不通过 Master 节点读写文件数据。反之，客户端向 Master 节点询问它应该联系的 Chunk 服务器。
  客户端将这些元数据信息缓存一段时间，后续的操作将直接和 Chunk 服务器进行数据读写操作。

##### Chunk 的大小

对于 GFS 而言，Chunk 的大小是一个比较重要的参数，而 GFS 选择了使用 64MB 作为 Chunk 的大小。

较大的 Chunk 主要带来了如下几个好处：

1. 降低客户端与 Master 通信的频率
2. 增大客户端进行操作时这些操作落到同一个 Chunk 上的概率
3. 减少 Master 所要保存的元数据的体积

不过，较大的 Chunk 会使得小文件占据额外的存储空间；一般的小文件通常只会占据一个 Chunk，这些 Chunk 也容易成为系统的负载热点。但正如之前所设想的需求那样，这样的文件在 Google 的场景下不是普遍存在的，这样的问题并未在 Google 中真正出现过。即便真的出现了，也可以通过提升这类文件的 Replica 数量来将负载进行均衡。

##### 元数据

Master 服务器存储 3 种主要类型的元数据：

1. 文件和 Chunk 的命名空间
2. 文件和 Chunk 的对应关系
3. 每个 Chunk 副本的存放地点

* **所有的元数据都保存在 Master 服务器的内存中**。前两种类型的元数据同时也会以记录变更日志的方式记录在操作系统的系统日志文件中，日志文件存储在本地磁盘上，同时日志会被复制到其它的远程Master服务上。
* **Master 服务器不会持久保存 Chunk 位置信息**。Master服务器在启动时，或者有新的 Chunk 服务器加入时，向各个 Chunk 服务器**轮询**它们所存储的 Chunk 的信息。

##### 操作日志

​	操作日志包含了关键的元数据变更历史记录。**操作日志是元数据唯一的持久化存储记录**，它也作为判断同步操作顺序的逻辑时间基线。必须确保日志文件的完整，确保**只有在元数据的变化被持久化后，日志才对客户端是可见的**。否则，即使 Chunk 本身没有出现任何问题，我们仍有可能丢失整个文件系统，或者丢失客户端最近的操作。

​	Master 服务器在灾难恢复时，通过**重演操作日志把文件系统恢复到最近的状态**。Master 服务器在日志增长到一定量时对系统状态做一次 Checkpoint，将所有的状态数据写入一个 Checkpoint 文件。在灾难恢复的时候，Master 服务器就通过从磁盘上读取这个Checkpoint 文件，以及重演 Checkpoint 之后的有限个日志文件就能够恢复系统。

​	Master 服务器恢复只需要最新的 Checkpoint 文件和后续的日志文件。

##### 一致性模型

文件命名空间的修改（例如，文件创建）是原子性的。它们仅由 Master 节点的控制，命名空间锁提供了
原子性和正确性的保障，Master 节点的操作日志定义了这些操作在全局的顺序。

数据修改后文件 region的状态取决于操作的类型、成功与否、以及是否存在同步的修改。下表总结了各种操作
的结果。

![屏幕快照 2018-11-14 下午9.28.48.png](https://i.loli.net/2018/11/14/5bec231e44d06.png)

- 客户端读取不同的 Replica 时可能会读取到不同的内容，那这部分文件是**不一致**的（Inconsistent）
- 所有客户端无论读取哪个 Replica 都会读取到相同的内容，那这部分文件就是**一致**的（Consistent）
- 所有客户端都能看到上一次修改的所有完整内容，且这部分文件是一致的，那么我们说这部分文件是**确定的（Defined）**

在修改后，一个文件的当前状态将取决于此次修改的类型以及修改是否成功。具体来说：

- 如果一次写入操作成功且没有与其他并发的写入操作发生重叠，那这部分的文件是**确定**的（同时也是一致的）
- 如果有若干个写入操作并发地执行成功，那么这部分文件会是**一致**的但会是**不确定**的：在这种情况下，客户端所能看到的数据通常不能直接体现出其中的任何一次修改
- 失败的写入操作会让文件进入**不一致**的状态

GFS 支持的文件数据修改数据包括两种：指定偏移值的数据写入（Write）以及数据追加（Record Append）。当写入时，指定的数据会被直接写入到客户端指定的偏移位置中，覆盖原有的数据。GFS 并未为该操作提供太多的一致性保证：如果不同的客户端并发地写入同一块文件区域，操作完成后这块区域的数据可能由各次写入的数据碎片所组成，即进入**不确定**的状态。

与写入操作不同，GFS 确保即便是在并发时，数据追加操作也是原子且 at least once（至少一次）的。操作完成后，GFS 会把实际写入的偏移值返回给客户端，该偏移值即代表包含所写入数据的**确定**的文件区域的起始位置。

在读取数据时，为了避免读入填充数据或是损坏的数据，数据在写入前往往会放入一些如校验和等元信息以用于验证其可用性，如此一来 GFS 的客户端 library 便可以在读取时自动跳过填充和损坏的数据。不过，鉴于数据追加操作的 at lease once 特性，客户端仍有可能读入重复的数据，此时只能由上层应用通过鉴别记录的唯一 ID 等信息来过滤重复数据了。

#### 三、系统交互

##### Leases和更改顺序

在客户端对某个 Chunk 做出修改时，GFS 为了能够处理不同的并发修改，会把该 Chunk 的 Lease 交给某个副本，使其成为 Primary：Primary 会负责为这些修改安排一个执行顺序，然后其他 Replica 便按照相同的顺序执行这些修改。

Chunk Lease 在初始时会有 60 秒的超时时间。在未超时前，Primary 可以向 Master 申请延长 Chunk Lease 的时间；必要时 Master 也可以直接撤回已分配的 Chunk Lease。

![屏幕快照 2018-11-14 下午9.43.57.png](https://i.loli.net/2018/11/14/5bec26a9c429c.png)

1. 客户端向 Master 询问目前哪个 Chunk Server 持有该 Chunk 的 Lease
2. Master 向客户端返回 Primary 和其他 Replica 的位置
3. 客户端将数据推送到所有的 Replica 上。Chunk Server 会把这些数据保存在缓冲区中，等待使用
4. 待所有 Replica 都接收到数据后，客户端发送写请求给 Primary。Primary 为来自各个客户端的修改操作安排连续的执行序列号，并按顺序地应用于其本地存储的数据
5. Primary 将写请求转发给其他 Secondary Replica，Replica 们按照相同的顺序应用这些修改
6. Secondary Replica 响应 Primary，示意自己已经完成操作
7. Primary 响应客户端，并返回该过程中发生的错误（若有）

值得注意的是，这个流程特意将数据流与控制流分开：客户端先向 Chunk Server 提交数据，再将写请求发往 Primary。这么做的好处在于 GFS 能够更好地利用网络带宽资源。

从上述步骤可见，控制流借由写请求从客户端流向 Primary，再流向其他 Secondary Replica。实际上，数据流以一条线性数据管道进行传递的：客户端会把数据上传到离自己最近的 Replica，该 Replica 在接收到数据后再转发给离自己最近的另一个 Replica，如此递归直到所有 Replica 都能接收到数据，如此一来便能够利用上每台机器的所有出口带宽。

为了充分利用每台机器的带宽，**数据沿着一个 Chunk 服务器链顺序的推送**，而不是以其它拓扑形式分散推送（例如，树型拓扑结构）。线性推送模式下，每台机器所有的出口带宽都用于以最快的速度传输数据，而不是在多个接受者之间分配带宽。

每台机器都尽量的在网络拓扑中选择一台还没有接收到数据的、离自己最近的机器作为目标推送数据。GFS利用基于 TCP 连接的、管道式数据推送方式来最小化延迟。Chunk 服务器接收到数据后，马上开始向前推送。

##### 文件快照

GFS 还提供了文件快照操作，可为指定的文件或目录创建一个副本。

快照操作的实现采用了写时复制（Copy on Write）的思想：

1. 在 Master 接收到快照请求后，它首先会撤回这些 Chunk 的 Lease，以让接下来其他客户端对这些 Chunk 进行写入时都会需要请求 Master 获知 Primary 的位置，Master 便可利用这个机会创建新的 Chunk
2. 当 Chunk Lease 撤回或失效后，Master 会先写入日志，然后对自己管理的命名空间进行复制操作，复制产生的新记录指向原本的 Chunk
3. 当有客户端尝试对这些 Chunk 进行写入时，Master 会注意到这个 Chunk 的引用计数大于 1。此时，Master 会为即将产生的新 Chunk 生成一个 Handle，然后通知所有持有这些 Chunk 的 Chunk Server 在本地复制出一个新的 Chunk，应用上新的 Handle，然后再返回给客户端

#### 四、Master节点的操作

##### NameSpace管理和锁

Master 节点的很多操作会花费很长的时间，延缓了其它的 Master 节点的操作。因此，GFS允许多个
操作同时进行，使用名称空间的 region 上的锁来保证执行的正确顺序。

* 在逻辑上，GFS 的名称空间就是一个全路径和元数据映射关系的查找表。利用前缀压缩，这个表可以高效的存储在内存中。在存储名称空间的树型结构上，**每个节点（绝对路径的文件名或绝对路径的目录名）都有一个关联的读写锁。**每个 Master 节点的操作在开始之前都要获得一系列的锁。

每个 Master 节点的操作在开始之前都要获得一系列的锁。通常情况下，如果一个操作涉及/d1/d2/…/dn/leaf，那么操作首先要获得目录/d1，/d1/d2，…，/d1/d2/…/dn 的读锁，以及/d1/d2/…/dn/leaf 的读写锁。注意，根据操作的不同，leaf 可以是一个文件，也可以是一个目录。

* 采用这种锁方案的优点是支持对同一目录的并行操作。比如，可以再同一个目录下同时创建多个文件：
  每一个操作都获取一个目录名的上的读取锁和文件名上的写入锁。**目录名的读取锁足以的防止目录被删除、**
  **改名以及被快照，文件名的写入锁序列化文件创建操作，确保不会多次创建同名的文件**。

* 名称空间可能有很多节点，读写锁采用惰性分配策略，在不再使用的时候立刻被删除。同样，锁的获取也要依据一个全局一致的顺序来避免死锁：首先按名称空间的层次排序，在同一个层次内按字典顺序排序。

##### 副本的位置

Chunk 副本位置选择的策略服务两大目标：**最大化数据可靠性和可用性，最大化网络带宽利用率。**我们必须在多个机架间分布储存Chunk的副本。这保证Chunk的一些副本在整个机架被破坏或掉线（比如，共享资源，如电源或者网络交换机造成的问题）的情况下依然存在且保持可用状态。这还意味着在网络流量方面，尤其是针对 Chunk 的读操作，能够有效利用多个机架的整合带宽。

* Chunk 的副本有三个用途：Chunk 创建，重新复制和重新负载均衡。

当 Master 节点创建一个 Chunk 时，它会选择在哪里放置初始的空的副本。Master 节点会考虑几个因素：

1. 希望在低于平均硬盘使用率的 Chunk 服务器上存储新的副本。这样的做法最终能够平衡 Chunk服务器之间的硬盘使用率。
2. 希望限制在每个 Chunk 服务器上“最近”的 Chunk 创建操作的次数。虽然创建操作本身是廉价的，但是创建操作也意味着随之会有大量的写入数据的操作，因为 Chunk 在 Writer 真正写入数据的时候才被创建，而在我们的“追加一次，读取多次”的工作模式下，Chunk 一旦写入成功之后就会变为只读的了。
3. 如上所述，我们希望把 Chunk 的副本分布在多个机架之间。

* 当 Chunk 的有效副本数量少于用户指定的复制因数的时候，Master 节点会重新复制它。。每个需要被重新复制的 Chunk 都会根据几个因素进行排序。**一个因素是 Chunk 现有副本数量和复制因数相差多少。另外，GFS优先重新复制活跃（live）文件的 Chunk 而不是最近刚被删除的文件的 Chunk。为了最小化失效的 Chunk 对正在运行的应用程序的影响，我们提高会阻塞客户机程序处理流程的 Chunk 的优先级。**

* 选择新副本的位置的策略和创建时类似：平衡硬盘使用率、限制同一台 Chunk 服务器上的正在进行的克隆操作的数量、在机架间分布副本。

##### 垃圾回收

* GFS 在文件删除后不会立刻回收可用的物理空间。GFS 空间回收采用惰性的策略，**只在文件和 Chunk 级的常规垃圾收集时进行。**
* 当一个文件被应用程序删除时，Master 节点象对待其它修改操作一样，立刻把删除操作以日志的方式记录下来。但是，Master 节点并不马上回收资源，而是把文件名改为一个包含删除时间戳的、隐藏的名字。Master 节点对文件系统命名空间做常规扫描的时候，它会删除所有三天前的隐藏文件。直到文件被真正删除，**它们仍旧可以用新的特殊的名字读取，也可以通过把隐藏文件改名为正常显示的文件名的方式“反删除”**。当隐藏文件被从名称空间中删除，Master 服务器内存中保存的这个文件的相关元数据才会被删除。

采用这种删除机制主要有如下三点好处：

1. 对于大规模的分布式系统来说，这样的机制更为**可靠**：在 Chunk 创建时，创建操作可能在某些 Chunk Server 上成功了，在其他 Chunk Server 上失败了，这导致某些 Chunk Server 上可能存在 Master 不知道的 Replica。除此以外，删除 Replica 的请求可能会发送失败，Master 会需要记得尝试重发。相比之下，由 Chunk Server 主动地删除 Replica 能够以一种更为统一的方式解决以上的问题
2. 这样的删除机制将存储回收过程与 Master 日常的周期扫描过程合并在了一起，这就使得这些操作可以以批的形式进行处理，以减少资源损耗；除外，这样也得以让 Master 选择在相对空闲的时候进行这些操作
3. 用户发送删除请求和数据被实际删除之间的延迟也有效避免了用户误操作的问题

#### 五、容错和诊断

##### 高可用性

GFS使用**快速恢复和复制**保证整个系统的高可用性

* 快速恢复：不管 Master 服务器和 Chunk 服务器是如何关闭的，它们都被设计为可以在数秒钟内恢复它们的状态并重新启动。GFS并不区分正常关闭和异常关闭。
* Chunk复制：当有 Chunk 服务器离线了，或者通过 Chksum 校验发现了已经损坏的数据，Master 节点通过克隆已有的副本保证每个 Chunk 都被完整复制

* Master服务器的复制：Master 服务器所有的操作日志和checkpoint 文件都被复制到多台机器上。对 Master 服务器状态的修改操作能够提交成功的前提是，操作日志写入到 Master 服务器的备节点和本机的磁盘。如果 Master 进程所在的机器或者磁盘失效了，处于 GFS 系统外部的监控进程会在其它的存有完整操作日志的机器上启动一个新的 Master 进程。

##### 数据完整性

每个 Chunk 服务器都使用 Checksum 来检查保存的数据是否损坏。

GFS 允许有歧义的副本存在：GFS 修改操作的语义，特别是早先讨论过的原子纪录追加的操作，副本不是 byte-wise 完全一致的。因此，**每个 Chunk 服务器必须独立维护 Checksum 来校验自己的副本的完整性。**

对于读操作来说，在把数据返回给客户端或者其它的 Chunk 服务器之前，Chunk 服务器会校验读取操作涉及的范围内的块的 Checksum。因此 **Chunk 服务器不会把错误数据传递到其它的机器上**。如果发生某个块的Checksum 不正确，Chunk 服务器返回给请求者一个错误信息，并且通知 Master 服务器这个错误。作为回应，请求者应当从其它副本读取数据，Master 服务器也会从其它副本克隆数据进行恢复。当一个新的副本就绪后，Master 服务器通知副本错误的 Chunk 服务器删掉错误的副本。